[2021-12-18 10:20:08,058] {taskinstance.py:1035} INFO - Dependencies all met for <TaskInstance: scraping_daft_dublin.scrap_data scheduled__2021-12-17T09:30:00+00:00 [queued]>
[2021-12-18 10:20:08,164] {taskinstance.py:1035} INFO - Dependencies all met for <TaskInstance: scraping_daft_dublin.scrap_data scheduled__2021-12-17T09:30:00+00:00 [queued]>
[2021-12-18 10:20:08,174] {taskinstance.py:1241} INFO - 
--------------------------------------------------------------------------------
[2021-12-18 10:20:08,184] {taskinstance.py:1242} INFO - Starting attempt 1 of 4
[2021-12-18 10:20:08,191] {taskinstance.py:1243} INFO - 
--------------------------------------------------------------------------------
[2021-12-18 10:20:08,289] {taskinstance.py:1262} INFO - Executing <Task(SparkSubmitOperator): scrap_data> on 2021-12-17 09:30:00+00:00
[2021-12-18 10:20:08,342] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'scraping_daft_dublin', 'scrap_data', 'scheduled__2021-12-17T09:30:00+00:00', '--job-id', '2982', '--raw', '--subdir', 'DAGS_FOLDER/scraping_daft_dublin.py', '--cfg-path', '/tmp/tmp5dna2cro', '--error-file', '/tmp/tmp7nis0dpo']
[2021-12-18 10:20:08,359] {standard_task_runner.py:77} INFO - Job 2982: Subtask scrap_data
[2021-12-18 10:20:08,303] {standard_task_runner.py:52} INFO - Started process 128 to run task
[2021-12-18 10:20:09,293] {logging_mixin.py:109} INFO - Running <TaskInstance: scraping_daft_dublin.scrap_data scheduled__2021-12-17T09:30:00+00:00 [running]> on host cf1c44b514e9
[2021-12-18 10:20:10,390] {taskinstance.py:1429} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_EMAIL=clement.liscoet@gmail.com
AIRFLOW_CTX_DAG_OWNER=clemoni
AIRFLOW_CTX_DAG_ID=scraping_daft_dublin
AIRFLOW_CTX_TASK_ID=scrap_data
AIRFLOW_CTX_EXECUTION_DATE=2021-12-17T09:30:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-12-17T09:30:00+00:00
[2021-12-18 10:20:10,549] {base.py:79} INFO - Using connection to: id: spark_default. Host: spark://spark:7077, Port: None, Schema: , Login: ***, Password: ***, extra: {}
[2021-12-18 10:20:10,562] {spark_submit.py:360} INFO - Spark-Submit cmd: spark-submit --master spark://spark:7077 --conf spark.master=spark://spark:7077 --name scrap data --verbose /opt/***/jobs/get_ads_per_county.py dublin
[2021-12-18 10:20:32,860] {spark_submit.py:514} INFO - Using properties file: null
[2021-12-18 10:20:33,907] {spark_submit.py:514} INFO - WARNING: An illegal reflective access operation has occurred
[2021-12-18 10:20:33,909] {spark_submit.py:514} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/***/.local/lib/python3.6/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2021-12-18 10:20:33,910] {spark_submit.py:514} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2021-12-18 10:20:33,912] {spark_submit.py:514} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2021-12-18 10:20:33,913] {spark_submit.py:514} INFO - WARNING: All illegal access operations will be denied in a future release
[2021-12-18 10:20:34,415] {spark_submit.py:514} INFO - Parsed arguments:
[2021-12-18 10:20:34,416] {spark_submit.py:514} INFO - master                  spark://spark:7077
[2021-12-18 10:20:34,417] {spark_submit.py:514} INFO - deployMode              null
[2021-12-18 10:20:34,419] {spark_submit.py:514} INFO - executorMemory          null
[2021-12-18 10:20:34,420] {spark_submit.py:514} INFO - executorCores           null
[2021-12-18 10:20:34,421] {spark_submit.py:514} INFO - totalExecutorCores      null
[2021-12-18 10:20:34,422] {spark_submit.py:514} INFO - propertiesFile          null
[2021-12-18 10:20:34,423] {spark_submit.py:514} INFO - driverMemory            null
[2021-12-18 10:20:34,424] {spark_submit.py:514} INFO - driverCores             null
[2021-12-18 10:20:34,425] {spark_submit.py:514} INFO - driverExtraClassPath    null
[2021-12-18 10:20:34,426] {spark_submit.py:514} INFO - driverExtraLibraryPath  null
[2021-12-18 10:20:34,428] {spark_submit.py:514} INFO - driverExtraJavaOptions  null
[2021-12-18 10:20:34,429] {spark_submit.py:514} INFO - supervise               false
[2021-12-18 10:20:34,430] {spark_submit.py:514} INFO - queue                   null
[2021-12-18 10:20:34,432] {spark_submit.py:514} INFO - numExecutors            null
[2021-12-18 10:20:34,433] {spark_submit.py:514} INFO - files                   null
[2021-12-18 10:20:34,434] {spark_submit.py:514} INFO - pyFiles                 null
[2021-12-18 10:20:34,435] {spark_submit.py:514} INFO - archives                null
[2021-12-18 10:20:34,436] {spark_submit.py:514} INFO - mainClass               null
[2021-12-18 10:20:34,437] {spark_submit.py:514} INFO - primaryResource         file:/opt/***/jobs/get_ads_per_county.py
[2021-12-18 10:20:34,438] {spark_submit.py:514} INFO - name                    scrap data
[2021-12-18 10:20:34,439] {spark_submit.py:514} INFO - childArgs               [dublin]
[2021-12-18 10:20:34,440] {spark_submit.py:514} INFO - jars                    null
[2021-12-18 10:20:34,442] {spark_submit.py:514} INFO - packages                null
[2021-12-18 10:20:34,443] {spark_submit.py:514} INFO - packagesExclusions      null
[2021-12-18 10:20:34,444] {spark_submit.py:514} INFO - repositories            null
[2021-12-18 10:20:34,445] {spark_submit.py:514} INFO - verbose                 true
[2021-12-18 10:20:34,447] {spark_submit.py:514} INFO - 
[2021-12-18 10:20:34,448] {spark_submit.py:514} INFO - Spark properties used, including those specified through
[2021-12-18 10:20:34,449] {spark_submit.py:514} INFO - --conf and those from the properties file null:
[2021-12-18 10:20:34,451] {spark_submit.py:514} INFO - (spark.master,spark://spark:7077)
[2021-12-18 10:20:34,458] {spark_submit.py:514} INFO - 
[2021-12-18 10:20:34,460] {spark_submit.py:514} INFO - 
[2021-12-18 10:20:38,089] {spark_submit.py:514} INFO - Main class:
[2021-12-18 10:20:38,090] {spark_submit.py:514} INFO - org.apache.spark.deploy.PythonRunner
[2021-12-18 10:20:38,091] {spark_submit.py:514} INFO - Arguments:
[2021-12-18 10:20:38,092] {spark_submit.py:514} INFO - file:/opt/***/jobs/get_ads_per_county.py
[2021-12-18 10:20:38,093] {spark_submit.py:514} INFO - null
[2021-12-18 10:20:38,094] {spark_submit.py:514} INFO - dublin
[2021-12-18 10:20:38,095] {spark_submit.py:514} INFO - --verbose
[2021-12-18 10:20:38,115] {spark_submit.py:514} INFO - Spark config:
[2021-12-18 10:20:38,116] {spark_submit.py:514} INFO - (spark.app.name,scrap data)
[2021-12-18 10:20:38,118] {spark_submit.py:514} INFO - (spark.master,spark://spark:7077)
[2021-12-18 10:20:38,119] {spark_submit.py:514} INFO - (spark.submit.pyFiles,)
[2021-12-18 10:20:38,120] {spark_submit.py:514} INFO - (spark.submit.deployMode,client)
[2021-12-18 10:20:38,122] {spark_submit.py:514} INFO - Classpath elements:
[2021-12-18 10:20:38,123] {spark_submit.py:514} INFO - 
[2021-12-18 10:20:38,124] {spark_submit.py:514} INFO - 
[2021-12-18 10:20:38,125] {spark_submit.py:514} INFO - 
[2021-12-18 10:20:53,334] {spark_submit.py:514} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2021-12-18 10:20:53,657] {spark_submit.py:514} INFO - 21/12/18 10:20:53 INFO SparkContext: Running Spark version 3.2.0
[2021-12-18 10:20:54,306] {spark_submit.py:514} INFO - 21/12/18 10:20:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2021-12-18 10:20:55,764] {spark_submit.py:514} INFO - 21/12/18 10:20:55 INFO ResourceUtils: ==============================================================
[2021-12-18 10:20:55,766] {spark_submit.py:514} INFO - 21/12/18 10:20:55 INFO ResourceUtils: No custom resources configured for spark.driver.
[2021-12-18 10:20:55,781] {spark_submit.py:514} INFO - 21/12/18 10:20:55 INFO ResourceUtils: ==============================================================
[2021-12-18 10:20:55,783] {spark_submit.py:514} INFO - 21/12/18 10:20:55 INFO SparkContext: Submitted application: Get Ads per county
[2021-12-18 10:20:56,279] {spark_submit.py:514} INFO - 21/12/18 10:20:56 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2021-12-18 10:20:57,021] {spark_submit.py:514} INFO - 21/12/18 10:20:57 INFO ResourceProfile: Limiting resource is cpu
[2021-12-18 10:20:57,023] {spark_submit.py:514} INFO - 21/12/18 10:20:57 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2021-12-18 10:20:59,195] {spark_submit.py:514} INFO - 21/12/18 10:20:59 INFO SecurityManager: Changing view acls to: ***
[2021-12-18 10:20:59,205] {spark_submit.py:514} INFO - 21/12/18 10:20:59 INFO SecurityManager: Changing modify acls to: ***
[2021-12-18 10:20:59,230] {spark_submit.py:514} INFO - 21/12/18 10:20:59 INFO SecurityManager: Changing view acls groups to:
[2021-12-18 10:20:59,248] {spark_submit.py:514} INFO - 21/12/18 10:20:59 INFO SecurityManager: Changing modify acls groups to:
[2021-12-18 10:20:59,263] {spark_submit.py:514} INFO - 21/12/18 10:20:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2021-12-18 10:21:04,503] {spark_submit.py:514} INFO - 21/12/18 10:21:04 INFO Utils: Successfully started service 'sparkDriver' on port 39457.
[2021-12-18 10:21:04,862] {spark_submit.py:514} INFO - 21/12/18 10:21:04 INFO SparkEnv: Registering MapOutputTracker
[2021-12-18 10:21:05,587] {spark_submit.py:514} INFO - 21/12/18 10:21:05 INFO SparkEnv: Registering BlockManagerMaster
[2021-12-18 10:21:06,288] {spark_submit.py:514} INFO - 21/12/18 10:21:06 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2021-12-18 10:21:06,316] {spark_submit.py:514} INFO - 21/12/18 10:21:06 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2021-12-18 10:21:06,466] {spark_submit.py:514} INFO - 21/12/18 10:21:06 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2021-12-18 10:21:07,642] {spark_submit.py:514} INFO - 21/12/18 10:21:07 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e755edae-7828-419e-bce2-5eeb83e280df
[2021-12-18 10:21:07,821] {spark_submit.py:514} INFO - 21/12/18 10:21:07 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2021-12-18 10:21:07,916] {spark_submit.py:514} INFO - 21/12/18 10:21:07 INFO SparkEnv: Registering OutputCommitCoordinator
[2021-12-18 10:21:11,535] {spark_submit.py:514} INFO - 21/12/18 10:21:11 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2021-12-18 10:21:11,537] {spark_submit.py:514} INFO - 21/12/18 10:21:11 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2021-12-18 10:21:11,539] {spark_submit.py:514} INFO - 21/12/18 10:21:11 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2021-12-18 10:21:11,597] {spark_submit.py:514} INFO - 21/12/18 10:21:11 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2021-12-18 10:21:11,611] {spark_submit.py:514} INFO - 21/12/18 10:21:11 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2021-12-18 10:21:12,064] {spark_submit.py:514} INFO - 21/12/18 10:21:12 INFO Utils: Successfully started service 'SparkUI' on port 4045.
[2021-12-18 10:21:13,876] {spark_submit.py:514} INFO - 21/12/18 10:21:13 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://cf1c44b514e9:4045
[2021-12-18 10:21:20,159] {spark_submit.py:514} INFO - 21/12/18 10:21:20 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark:7077...
[2021-12-18 10:21:21,383] {spark_submit.py:514} INFO - 21/12/18 10:21:21 INFO TransportClientFactory: Successfully created connection to spark/172.19.0.4:7077 after 640 ms (0 ms spent in bootstraps)
[2021-12-18 10:21:23,429] {spark_submit.py:514} INFO - 21/12/18 10:21:23 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20211218102123-0004
[2021-12-18 10:21:23,484] {spark_submit.py:514} INFO - 21/12/18 10:21:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37873.
[2021-12-18 10:21:23,486] {spark_submit.py:514} INFO - 21/12/18 10:21:23 INFO NettyBlockTransferService: Server created on cf1c44b514e9:37873
[2021-12-18 10:21:23,499] {spark_submit.py:514} INFO - 21/12/18 10:21:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2021-12-18 10:21:23,542] {spark_submit.py:514} INFO - 21/12/18 10:21:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, cf1c44b514e9, 37873, None)
[2021-12-18 10:21:23,623] {spark_submit.py:514} INFO - 21/12/18 10:21:23 INFO BlockManagerMasterEndpoint: Registering block manager cf1c44b514e9:37873 with 434.4 MiB RAM, BlockManagerId(driver, cf1c44b514e9, 37873, None)
[2021-12-18 10:21:23,897] {spark_submit.py:514} INFO - 21/12/18 10:21:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, cf1c44b514e9, 37873, None)
[2021-12-18 10:21:23,950] {spark_submit.py:514} INFO - 21/12/18 10:21:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, cf1c44b514e9, 37873, None)
[2021-12-18 10:21:40,086] {spark_submit.py:514} INFO - 21/12/18 10:21:40 INFO AsyncEventQueue: Process of event SparkListenerResourceProfileAdded(Profile: id = 0, executor resources: cores -> name: cores, amount: 1, script: , vendor: ,memory -> name: memory, amount: 1024, script: , vendor: ,offHeap -> name: offHeap, amount: 0, script: , vendor: , task resources: cpus -> name: cpus, amount: 1.0) by listener HeartbeatReceiver took 2.4567082s.
[2021-12-18 10:21:40,097] {spark_submit.py:514} INFO - 21/12/18 10:21:40 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2021-12-18 10:21:41,537] {spark_submit.py:514} INFO - 21/12/18 10:21:41 INFO AsyncEventQueue: Process of event SparkListenerResourceProfileAdded(Profile: id = 0, executor resources: cores -> name: cores, amount: 1, script: , vendor: ,memory -> name: memory, amount: 1024, script: , vendor: ,offHeap -> name: offHeap, amount: 0, script: , vendor: , task resources: cpus -> name: cpus, amount: 1.0) by listener AppStatusListener took 4.3668249s.
[2021-12-18 10:21:50,145] {spark_submit.py:514} INFO - 21/12/18 10:21:50 INFO AsyncEventQueue: Process of event SparkListenerBlockManagerAdded(1639822883604,BlockManagerId(driver, cf1c44b514e9, 37873, None),455501414,Some(455501414),Some(0)) by listener AppStatusListener took 6.9559917s.
[2021-12-18 10:22:05,842] {spark_submit.py:514} INFO - 21/12/18 10:22:05 INFO AsyncEventQueue: Process of event SparkListenerEnvironmentUpdate(Map(Spark Properties -> ArrayBuffer((spark.app.id,app-20211218102123-0004), (spark.app.name,Get Ads per county), (spark.app.startTime,1639822853071), (spark.driver.host,cf1c44b514e9), (spark.driver.port,39457), (spark.executor.id,driver), (spark.master,spark://spark:7077), (spark.rdd.compress,True), (spark.scheduler.mode,FIFO), (spark.serializer.objectStreamReset,100), (spark.submit.deployMode,client), (spark.submit.pyFiles,)), Classpath Entries -> Vector((/home/***/.local/lib/python3.6/site-packages/pyspark/conf,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/HikariCP-2.5.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/JLargeArrays-1.5.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/JTransforms-3.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/RoaringBitmap-0.9.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/ST4-4.0.4.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/activation-1.1.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/aircompressor-0.21.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/algebra_2.12-2.0.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/annotations-17.0.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/antlr-runtime-3.5.2.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/antlr4-runtime-4.8.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/aopalliance-repackaged-2.6.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/arpack-2.2.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/arpack_combined_all-0.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/arrow-format-2.0.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/arrow-memory-core-2.0.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/arrow-memory-netty-2.0.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/arrow-vector-2.0.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/audience-annotations-0.5.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/automaton-1.11-8.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/avro-1.10.2.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/avro-ipc-1.10.2.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/avro-mapred-1.10.2.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/blas-2.2.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/bonecp-0.8.0.RELEASE.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/breeze-macros_2.12-1.2.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/breeze_2.12-1.2.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/cats-kernel_2.12-2.1.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/chill-java-0.10.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/chill_2.12-0.10.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/commons-cli-1.2.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/commons-codec-1.15.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/commons-collections-3.2.2.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/commons-compiler-3.0.16.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/commons-compress-1.21.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/commons-crypto-1.1.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/commons-dbcp-1.4.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/commons-io-2.8.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/commons-lang-2.6.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/commons-lang3-3.12.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/commons-logging-1.1.3.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/commons-math3-3.4.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/commons-net-3.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/commons-pool-1.5.4.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/commons-text-1.6.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/compress-lzf-1.0.3.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/core-1.1.2.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/curator-client-2.13.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/curator-framework-2.13.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/curator-recipes-2.13.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/datanucleus-api-jdo-4.2.4.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/datanucleus-core-4.1.17.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/datanucleus-rdbms-4.1.19.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/derby-10.14.2.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/flatbuffers-java-1.9.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/generex-1.0.2.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/gson-2.2.4.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/guava-14.0.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/hadoop-client-api-3.3.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/hadoop-client-runtime-3.3.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/hadoop-shaded-guava-1.1.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/hadoop-yarn-server-web-proxy-3.3.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/hive-beeline-2.3.9.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/hive-cli-2.3.9.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/hive-common-2.3.9.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/hive-exec-2.3.9-core.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/hive-jdbc-2.3.9.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/hive-llap-common-2.3.9.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/hive-metastore-2.3.9.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/hive-serde-2.3.9.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/hive-service-rpc-3.1.2.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/hive-shims-0.23-2.3.9.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/hive-shims-2.3.9.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/hive-shims-common-2.3.9.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/hive-shims-scheduler-2.3.9.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/hive-storage-api-2.7.2.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/hive-vector-code-gen-2.3.9.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/hk2-api-2.6.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/hk2-locator-2.6.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/hk2-utils-2.6.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/htrace-core4-4.1.0-incubating.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/httpclient-4.5.13.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/httpcore-4.4.14.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/istack-commons-runtime-3.0.8.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/ivy-2.5.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/jackson-annotations-2.12.3.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/jackson-core-2.12.3.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/jackson-core-asl-1.9.13.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/jackson-databind-2.12.3.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/jackson-dataformat-yaml-2.12.3.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/jackson-datatype-jsr310-2.11.2.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/jackson-mapper-asl-1.9.13.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/jackson-module-scala_2.12-2.12.3.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/jakarta.annotation-api-1.3.5.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/jakarta.inject-2.6.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/jakarta.servlet-api-4.0.3.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/jakarta.validation-api-2.0.2.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/jakarta.ws.rs-api-2.1.6.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/jakarta.xml.bind-api-2.3.2.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/janino-3.0.16.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/javassist-3.25.0-GA.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/javax.jdo-3.2.0-m3.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/javolution-5.5.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/jaxb-api-2.2.11.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/jaxb-runtime-2.3.2.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/jcl-over-slf4j-1.7.30.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/jdo-api-3.0.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/jersey-client-2.34.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/jersey-common-2.34.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/jersey-container-servlet-2.34.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/jersey-container-servlet-core-2.34.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/jersey-hk2-2.34.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/jersey-server-2.34.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/jline-2.14.6.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/joda-time-2.10.10.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/jodd-core-3.5.2.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/jpam-1.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/json-1.8.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/json4s-ast_2.12-3.7.0-M11.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/json4s-core_2.12-3.7.0-M11.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/json4s-jackson_2.12-3.7.0-M11.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/json4s-scalap_2.12-3.7.0-M11.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/jsr305-3.0.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/jta-1.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/jul-to-slf4j-1.7.30.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/kryo-shaded-4.0.2.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/kubernetes-client-5.4.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/kubernetes-model-admissionregistration-5.4.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/kubernetes-model-apiextensions-5.4.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/kubernetes-model-apps-5.4.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/kubernetes-model-autoscaling-5.4.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/kubernetes-model-batch-5.4.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/kubernetes-model-certificates-5.4.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/kubernetes-model-common-5.4.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/kubernetes-model-coordination-5.4.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/kubernetes-model-core-5.4.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/kubernetes-model-discovery-5.4.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/kubernetes-model-events-5.4.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/kubernetes-model-extensions-5.4.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/kubernetes-model-flowcontrol-5.4.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/kubernetes-model-metrics-5.4.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/kubernetes-model-networking-5.4.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/kubernetes-model-node-5.4.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/kubernetes-model-policy-5.4.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/kubernetes-model-rbac-5.4.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/kubernetes-model-scheduling-5.4.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/kubernetes-model-storageclass-5.4.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/lapack-2.2.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/leveldbjni-all-1.8.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/libfb303-0.9.3.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/libthrift-0.12.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/log4j-1.2.17.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/logging-interceptor-3.12.12.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/lz4-java-1.7.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/macro-compat_2.12-1.1.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/mesos-1.4.0-shaded-protobuf.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/metrics-core-4.2.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/metrics-graphite-4.2.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/metrics-jmx-4.2.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/metrics-json-4.2.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/metrics-jvm-4.2.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/minlog-1.3.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/netty-all-4.1.68.Final.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/objenesis-2.6.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/okhttp-3.12.12.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/okio-1.14.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/opencsv-2.3.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/orc-core-1.6.11.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/orc-mapreduce-1.6.11.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/orc-shims-1.6.11.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/oro-2.0.8.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/osgi-resource-locator-1.0.3.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/paranamer-2.8.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/parquet-column-1.12.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/parquet-common-1.12.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/parquet-encoding-1.12.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/parquet-format-structures-1.12.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/parquet-hadoop-1.12.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/parquet-jackson-1.12.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/protobuf-java-2.5.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/py4j-0.10.9.2.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/pyrolite-4.30.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/rocksdbjni-6.20.3.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/scala-collection-compat_2.12-2.1.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/scala-compiler-2.12.15.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/scala-library-2.12.15.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/scala-parser-combinators_2.12-1.1.2.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/scala-reflect-2.12.15.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/scala-xml_2.12-1.2.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/shapeless_2.12-2.3.3.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/shims-0.9.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/slf4j-api-1.7.30.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/slf4j-log4j12-1.7.30.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/snakeyaml-1.27.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/snappy-java-1.1.8.4.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/spark-catalyst_2.12-3.2.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/spark-core_2.12-3.2.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/spark-graphx_2.12-3.2.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/spark-hive-thriftserver_2.12-3.2.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/spark-hive_2.12-3.2.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/spark-kubernetes_2.12-3.2.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/spark-kvstore_2.12-3.2.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/spark-launcher_2.12-3.2.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/spark-mesos_2.12-3.2.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/spark-mllib-local_2.12-3.2.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/spark-mllib_2.12-3.2.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/spark-network-common_2.12-3.2.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/spark-network-shuffle_2.12-3.2.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/spark-repl_2.12-3.2.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/spark-sketch_2.12-3.2.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/spark-sql_2.12-3.2.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/spark-streaming_2.12-3.2.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/spark-tags_2.12-3.2.0-tests.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/spark-tags_2.12-3.2.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/spark-yarn_2.12-3.2.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/spire-macros_2.12-0.17.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/spire-platform_2.12-0.17.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/spire-util_2.12-0.17.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/spire_2.12-0.17.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/stax-api-1.0.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/stream-2.9.6.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/super-csv-2.2.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/threeten-extra-1.5.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/tink-1.6.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/transaction-api-1.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/univocity-parsers-2.9.1.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/velocity-1.5.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/xbean-asm9-shaded-4.20.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/xz-1.8.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/zjsonpatch-0.3.0.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/zookeeper-3.6.2.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/zookeeper-jute-3.6.2.jar,System Classpath), (/home/***/.local/lib/python3.6/site-packages/pyspark/jars/zstd-jni-1.5.0-4.jar,System Classpath)), Hadoop Properties -> List((adl.feature.ownerandgroup.enableupn,false), (adl.http.timeout,-1), (dfs.client.ignore.namenode.default.kms.uri,false), (dfs.ha.fencing.ssh.connect-timeout,30000), (file.blocksize,67108864), (file.bytes-per-checksum,512), (file.client-write-packet-size,65536), (file.replication,1), (file.stream-buffer-size,4096), (fs.AbstractFileSystem.abfs.impl,org.apache.hadoop.fs.azurebfs.Abfs), (fs.AbstractFileSystem.abfss.impl,org.apache.hadoop.fs.azurebfs.Abfss), (fs.AbstractFileSystem.adl.impl,org.apache.hadoop.fs.adl.Adl), (fs.AbstractFileSystem.file.impl,org.apache.hadoop.fs.local.LocalFs), (fs.AbstractFileSystem.ftp.impl,org.apache.hadoop.fs.ftp.FtpFs), (fs.AbstractFileSystem.har.impl,org.apache.hadoop.fs.HarFs), (fs.AbstractFileSystem.hdfs.impl,org.apache.hadoop.fs.Hdfs), (fs.AbstractFileSystem.s3a.impl,org.apache.hadoop.fs.s3a.S3A), (fs.AbstractFileSystem.swebhdfs.impl,org.apache.hadoop.fs.SWebHdfs), (fs.AbstractFileSystem.viewfs.impl,org.apache.hadoop.fs.viewfs.ViewFs), (fs.AbstractFileSystem.wasb.impl,org.apache.hadoop.fs.azure.Wasb), (fs.AbstractFileSystem.wasbs.impl,org.apache.hadoop.fs.azure.Wasbs), (fs.AbstractFileSystem.webhdfs.impl,org.apache.hadoop.fs.WebHdfs), (fs.abfs.impl,org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem), (fs.abfss.impl,org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem), (fs.adl.impl,org.apache.hadoop.fs.adl.AdlFileSystem), (fs.adl.oauth2.access.token.provider.type,*********(redacted)), (fs.automatic.close,true), (fs.azure.authorization,false), (fs.azure.authorization.caching.enable,true), (fs.azure.local.sas.key.mode,false), (fs.azure.sas.expiry.period,90d), (fs.azure.saskey.usecontainersaskeyforallaccess,true), (fs.azure.secure.mode,false), (fs.azure.user.agent.prefix,unknown), (fs.client.resolve.remote.symlinks,true), (fs.client.resolve.topology.enabled,false), (fs.defaultFS,file:///), (fs.df.interval,60000), (fs.du.interval,600000), (fs.ftp.data.connection.mode,ACTIVE_LOCAL_DATA_CONNECTION_MODE), (fs.ftp.host,0.0.0.0), (fs.ftp.host.port,21), (fs.ftp.impl,org.apache.hadoop.fs.ftp.FTPFileSystem), (fs.ftp.timeout,0), (fs.ftp.transfer.mode,BLOCK_TRANSFER_MODE), (fs.getspaceused.jitterMillis,60000), (fs.har.impl.disable.cache,true), (fs.permissions.umask-mode,022), (fs.s3a.assumed.role.credentials.provider,org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider), (fs.s3a.assumed.role.session.duration,30m), (fs.s3a.attempts.maximum,20), (fs.s3a.aws.credentials.provider,
[2021-12-18 10:22:05,855] {spark_submit.py:514} INFO - org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider,
[2021-12-18 10:22:05,872] {spark_submit.py:514} INFO - org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider,
[2021-12-18 10:22:05,905] {spark_submit.py:514} INFO - com.amazonaws.auth.EnvironmentVariableCredentialsProvider,
[2021-12-18 10:22:05,907] {spark_submit.py:514} INFO - org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider
[2021-12-18 10:22:05,910] {spark_submit.py:514} INFO - ), (fs.s3a.block.size,32M), (fs.s3a.buffer.dir,${hadoop.tmp.dir}/s3a), (fs.s3a.change.detection.mode,server), (fs.s3a.change.detection.source,etag), (fs.s3a.change.detection.version.required,true), (fs.s3a.committer.abort.pending.uploads,true), (fs.s3a.committer.magic.enabled,true), (fs.s3a.committer.name,file), (fs.s3a.committer.staging.conflict-mode,append), (fs.s3a.committer.staging.tmp.path,tmp/staging), (fs.s3a.committer.staging.unique-filenames,true), (fs.s3a.committer.threads,8), (fs.s3a.connection.establish.timeout,5000), (fs.s3a.connection.maximum,48), (fs.s3a.connection.request.timeout,0), (fs.s3a.connection.ssl.enabled,true), (fs.s3a.connection.timeout,200000), (fs.s3a.delegation.tokens.enabled,*********(redacted)), (fs.s3a.downgrade.syncable.exceptions,true), (fs.s3a.endpoint,s3.amazonaws.com), (fs.s3a.etag.checksum.enabled,false), (fs.s3a.executor.capacity,16), (fs.s3a.fast.upload.active.blocks,4), (fs.s3a.fast.upload.buffer,disk), (fs.s3a.impl,org.apache.hadoop.fs.s3a.S3AFileSystem), (fs.s3a.list.version,2), (fs.s3a.max.total.tasks,32), (fs.s3a.metadatastore.authoritative,false), (fs.s3a.metadatastore.fail.on.write.error,true), (fs.s3a.metadatastore.impl,org.apache.hadoop.fs.s3a.s3guard.NullMetadataStore), (fs.s3a.metadatastore.metadata.ttl,15m), (fs.s3a.multiobjectdelete.enable,true), (fs.s3a.multipart.purge,false), (fs.s3a.multipart.purge.age,86400), (fs.s3a.multipart.size,64M), (fs.s3a.multipart.threshold,128M), (fs.s3a.paging.maximum,5000), (fs.s3a.path.style.access,false), (fs.s3a.readahead.range,64K), (fs.s3a.retry.interval,500ms), (fs.s3a.retry.limit,7), (fs.s3a.retry.throttle.interval,100ms), (fs.s3a.retry.throttle.limit,20), (fs.s3a.s3guard.cli.prune.age,86400000), (fs.s3a.s3guard.consistency.retry.interval,2s), (fs.s3a.s3guard.consistency.retry.limit,7), (fs.s3a.s3guard.ddb.background.sleep,25ms), (fs.s3a.s3guard.ddb.max.retries,9), (fs.s3a.s3guard.ddb.table.capacity.read,0), (fs.s3a.s3guard.ddb.table.capacity.write,0), (fs.s3a.s3guard.ddb.table.create,false), (fs.s3a.s3guard.ddb.table.sse.enabled,false), (fs.s3a.s3guard.ddb.throttle.retry.interval,100ms), (fs.s3a.select.enabled,true), (fs.s3a.select.errors.include.sql,false), (fs.s3a.select.input.compression,none), (fs.s3a.select.input.csv.comment.marker,#), (fs.s3a.select.input.csv.field.delimiter,,), (fs.s3a.select.input.csv.header,none), (fs.s3a.select.input.csv.quote.character,"), (fs.s3a.select.input.csv.quote.escape.character,\\), (fs.s3a.select.input.csv.record.delimiter,\n), (fs.s3a.select.output.csv.field.delimiter,,), (fs.s3a.select.output.csv.quote.character,"), (fs.s3a.select.output.csv.quote.escape.character,\\), (fs.s3a.select.output.csv.quote.fields,always), (fs.s3a.select.output.csv.record.delimiter,\n), (fs.s3a.socket.recv.buffer,8192), (fs.s3a.socket.send.buffer,8192), (fs.s3a.ssl.channel.mode,default_jsse), (fs.s3a.threads.keepalivetime,60), (fs.s3a.threads.max,64), (fs.swift.impl,org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem), (fs.trash.checkpoint.interval,0), (fs.trash.interval,0), (fs.viewfs.overload.scheme.target.abfs.impl,org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem), (fs.viewfs.overload.scheme.target.abfss.impl,org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem), (fs.viewfs.overload.scheme.target.file.impl,org.apache.hadoop.fs.LocalFileSystem), (fs.viewfs.overload.scheme.target.ftp.impl,org.apache.hadoop.fs.ftp.FTPFileSystem), (fs.viewfs.overload.scheme.target.hdfs.impl,org.apache.hadoop.hdfs.DistributedFileSystem), (fs.viewfs.overload.scheme.target.http.impl,org.apache.hadoop.fs.http.HttpFileSystem), (fs.viewfs.overload.scheme.target.https.impl,org.apache.hadoop.fs.http.HttpsFileSystem), (fs.viewfs.overload.scheme.target.o3fs.impl,org.apache.hadoop.fs.ozone.OzoneFileSystem), (fs.viewfs.overload.scheme.target.ofs.impl,org.apache.hadoop.fs.ozone.RootedOzoneFileSystem), (fs.viewfs.overload.scheme.target.oss.impl,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem), (fs.viewfs.overload.scheme.target.s3a.impl,org.apache.hadoop.fs.s3a.S3AFileSystem), (fs.viewfs.overload.scheme.target.swebhdfs.impl,org.apache.hadoop.hdfs.web.SWebHdfsFileSystem), (fs.viewfs.overload.scheme.target.swift.impl,org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem), (fs.viewfs.overload.scheme.target.wasb.impl,org.apache.hadoop.fs.azure.NativeAzureFileSystem), (fs.viewfs.overload.scheme.target.webhdfs.impl,org.apache.hadoop.hdfs.web.WebHdfsFileSystem), (fs.viewfs.rename.strategy,SAME_MOUNTPOINT), (fs.wasb.impl,org.apache.hadoop.fs.azure.NativeAzureFileSystem), (fs.wasbs.impl,org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure), (ftp.blocksize,67108864), (ftp.bytes-per-checksum,512), (ftp.client-write-packet-size,65536), (ftp.replication,3), (ftp.stream-buffer-size,4096), (ha.failover-controller.active-standby-elector.zk.op.retries,3), (ha.failover-controller.cli-check.rpc-timeout.ms,20000), (ha.failover-controller.graceful-fence.connection.retries,1), (ha.failover-controller.graceful-fence.rpc-timeout.ms,5000), (ha.failover-controller.new-active.rpc-timeout.ms,60000), (ha.health-monitor.check-interval.ms,1000), (ha.health-monitor.connect-retry-interval.ms,1000), (ha.health-monitor.rpc-timeout.ms,45000), (ha.health-monitor.rpc.connect.max.retries,1), (ha.health-monitor.sleep-after-disconnect.ms,1000), (ha.zookeeper.acl,world:anyone:rwcda), (ha.zookeeper.parent-znode,/hadoop-ha), (ha.zookeeper.session-timeout.ms,10000), (hadoop.caller.context.enabled,false), (hadoop.caller.context.max.size,128), (hadoop.caller.context.signature.max.size,40), (hadoop.common.configuration.version,3.0.0), (hadoop.domainname.resolver.impl,org.apache.hadoop.net.DNSDomainNameResolver), (hadoop.http.authentication.kerberos.keytab,${user.home}/hadoop.keytab), (hadoop.http.authentication.kerberos.principal,HTTP/_HOST@LOCALHOST), (hadoop.http.authentication.signature.secret.file,*********(redacted)), (hadoop.http.authentication.simple.anonymous.allowed,true), (hadoop.http.authentication.token.validity,*********(redacted)), (hadoop.http.authentication.type,simple), (hadoop.http.cross-origin.allowed-headers,X-Requested-With,Content-Type,Accept,Origin), (hadoop.http.cross-origin.allowed-methods,GET,POST,HEAD), (hadoop.http.cross-origin.allowed-origins,*), (hadoop.http.cross-origin.enabled,false), (hadoop.http.cross-origin.max-age,1800), (hadoop.http.filter.initializers,org.apache.hadoop.http.lib.StaticUserWebFilter), (hadoop.http.idle_timeout.ms,60000), (hadoop.http.logs.enabled,true), (hadoop.http.sni.host.check.enabled,false), (hadoop.http.staticuser.user,dr.who), (hadoop.jetty.logs.serve.aliases,true), (hadoop.kerberos.keytab.login.autorenewal.enabled,false), (hadoop.kerberos.kinit.command,kinit), (hadoop.kerberos.min.seconds.before.relogin,60), (hadoop.metrics.jvm.use-thread-mxbean,false), (hadoop.prometheus.endpoint.enabled,false), (hadoop.registry.jaas.context,Client), (hadoop.registry.secure,false), (hadoop.registry.system.acls,sasl:yarn@, sasl:mapred@, sasl:hdfs@), (hadoop.registry.zk.connection.timeout.ms,15000), (hadoop.registry.zk.quorum,localhost:2181), (hadoop.registry.zk.retry.ceiling.ms,60000), (hadoop.registry.zk.retry.interval.ms,1000), (hadoop.registry.zk.retry.times,5), (hadoop.registry.zk.root,/registry), (hadoop.registry.zk.session.timeout.ms,60000), (hadoop.rpc.protection,authentication), (hadoop.rpc.socket.factory.class.default,org.apache.hadoop.net.StandardSocketFactory), (hadoop.security.auth_to_local.mechanism,hadoop), (hadoop.security.authentication,simple), (hadoop.security.authorization,false), (hadoop.security.credential.clear-text-fallback,true), (hadoop.security.crypto.buffer.size,8192), (hadoop.security.crypto.cipher.suite,AES/CTR/NoPadding), (hadoop.security.crypto.codec.classes.aes.ctr.nopadding,org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec, org.apache.hadoop.crypto.JceAesCtrCryptoCodec), (hadoop.security.dns.log-slow-lookups.enabled,false), (hadoop.security.dns.log-slow-lookups.threshold.ms,1000), (hadoop.security.group.mapping,org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback), (hadoop.security.group.mapping.ldap.connection.timeout.ms,60000), (hadoop.security.group.mapping.ldap.conversion.rule,none), (hadoop.security.group.mapping.ldap.directory.search.timeout,10000), (hadoop.security.group.mapping.ldap.num.attempts,3), (hadoop.security.group.mapping.ldap.num.attempts.before.failover,3), (hadoop.security.group.mapping.ldap.posix.attr.gid.name,gidNumber), (hadoop.security.group.mapping.ldap.posix.attr.uid.name,uidNumber), (hadoop.security.group.mapping.ldap.read.timeout.ms,60000), (hadoop.security.group.mapping.ldap.search.attr.group.name,cn), (hadoop.security.group.mapping.ldap.search.attr.member,member), (hadoop.security.group.mapping.ldap.search.filter.group,(objectClass=group)), (hadoop.security.group.mapping.ldap.search.filter.user,(&(objectClass=user)(sAMAccountName={0}))), (hadoop.security.group.mapping.ldap.search.group.hierarchy.levels,0), (hadoop.security.group.mapping.ldap.ssl,false), (hadoop.security.group.mapping.providers.combined,true), (hadoop.security.groups.cache.background.reload,false), (hadoop.security.groups.cache.background.reload.threads,3), (hadoop.security.groups.cache.secs,300), (hadoop.security.groups.cache.warn.after.ms,5000), (hadoop.security.groups.negative-cache.secs,30), (hadoop.security.groups.shell.command.timeout,0s), (hadoop.security.instrumentation.requires.admin,false), (hadoop.security.java.secure.random.algorithm,SHA1PRNG), (hadoop.security.key.default.bitlength,128), (hadoop.security.key.default.cipher,AES/CTR/NoPadding), (hadoop.security.kms.client.authentication.retry-count,1), (hadoop.security.kms.client.encrypted.key.cache.expiry,43200000), (hadoop.security.kms.client.encrypted.key.cache.low-watermark,0.3f), (hadoop.security.kms.client.encrypted.key.cache.num.refill.threads,2), (hadoop.security.kms.client.encrypted.key.cache.size,500), (hadoop.security.kms.client.failover.sleep.base.millis,100), (hadoop.security.kms.client.failover.sleep.max.millis,2000), (hadoop.security.kms.client.timeout,60), (hadoop.security.random.device.file.path,/dev/urandom), (hadoop.security.secure.random.impl,org.apache.hadoop.crypto.random.OpensslSecureRandom), (hadoop.security.sensitive-config-keys,*********(redacted)), (hadoop.security.uid.cache.secs,14400), (hadoop.service.shutdown.timeout,30s), (hadoop.shell.missing.defaultFs.warning,false), (hadoop.shell.safely.delete.limit.num.files,100), (hadoop.ssl.client.conf,ssl-client.xml), (hadoop.ssl.enabled,false), (hadoop.ssl.enabled.protocols,TLSv1.2), (hadoop.ssl.hostname.verifier,DEFAULT), (hadoop.ssl.keystores.factory.class,org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory), (hadoop.ssl.require.client.cert,false), (hadoop.ssl.server.conf,ssl-server.xml), (hadoop.system.tags,YARN,HDFS,NAMENODE,DATANODE,REQUIRED,SECURITY,KERBEROS,PERFORMANCE,CLIENT
[2021-12-18 10:22:05,921] {spark_submit.py:514} INFO - ,SERVER,DEBUG,DEPRECATED,COMMON,OPTIONAL), (hadoop.tags.system,YARN,HDFS,NAMENODE,DATANODE,REQUIRED,SECURITY,KERBEROS,PERFORMANCE,CLIENT
[2021-12-18 10:22:05,925] {spark_submit.py:514} INFO - ,SERVER,DEBUG,DEPRECATED,COMMON,OPTIONAL), (hadoop.tmp.dir,/tmp/hadoop-${user.name}), (hadoop.user.group.static.mapping.overrides,dr.who=;), (hadoop.util.hash.type,murmur), (hadoop.workaround.non.threadsafe.getpwuid,true), (hadoop.zk.acl,world:anyone:rwcda), (hadoop.zk.num-retries,1000), (hadoop.zk.retry-interval-ms,1000), (hadoop.zk.timeout-ms,10000), (io.bytes.per.checksum,512), (io.compression.codec.bzip2.library,system-native), (io.erasurecode.codec.rs-legacy.rawcoders,rs-legacy_java), (io.erasurecode.codec.rs.rawcoders,rs_native,rs_java), (io.erasurecode.codec.xor.rawcoders,xor_native,xor_java), (io.file.buffer.size,65536), (io.map.index.interval,128), (io.map.index.skip,0), (io.mapfile.bloom.error.rate,0.005), (io.mapfile.bloom.size,1048576), (io.seqfile.compress.blocksize,1000000), (io.seqfile.local.dir,${hadoop.tmp.dir}/io/local), (io.serializations,org.apache.hadoop.io.serializer.WritableSerialization, org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization, org.apache.hadoop.io.serializer.avro.AvroReflectSerialization), (io.skip.checksum.errors,false), (ipc.[port_number].backoff.enable,false), (ipc.[port_number].callqueue.impl,java.util.concurrent.LinkedBlockingQueue), (ipc.[port_number].cost-provider.impl,org.apache.hadoop.ipc.DefaultCostProvider), (ipc.[port_number].decay-scheduler.backoff.responsetime.enable,false), (ipc.[port_number].decay-scheduler.backoff.responsetime.thresholds,10s,20s,30s,40s), (ipc.[port_number].decay-scheduler.decay-factor,0.5), (ipc.[port_number].decay-scheduler.metrics.top.user.count,10), (ipc.[port_number].decay-scheduler.period-ms,5000), (ipc.[port_number].decay-scheduler.thresholds,13,25,50), (ipc.[port_number].faircallqueue.multiplexer.weights,8,4,2,1), (ipc.[port_number].identity-provider.impl,org.apache.hadoop.ipc.UserIdentityProvider), (ipc.[port_number].scheduler.impl,org.apache.hadoop.ipc.DefaultRpcScheduler), (ipc.[port_number].scheduler.priority.levels,4), (ipc.[port_number].weighted-cost.handler,1), (ipc.[port_number].weighted-cost.lockexclusive,100), (ipc.[port_number].weighted-cost.lockfree,1), (ipc.[port_number].weighted-cost.lockshared,10), (ipc.[port_number].weighted-cost.response,1), (ipc.client.bind.wildcard.addr,false), (ipc.client.connect.max.retries,10), (ipc.client.connect.max.retries.on.timeouts,45), (ipc.client.connect.retry.interval,1000), (ipc.client.connect.timeout,20000), (ipc.client.connection.maxidletime,10000), (ipc.client.fallback-to-simple-auth-allowed,false), (ipc.client.idlethreshold,4000), (ipc.client.kill.max,10), (ipc.client.low-latency,false), (ipc.client.ping,true), (ipc.client.rpc-timeout.ms,0), (ipc.client.tcpnodelay,true), (ipc.maximum.data.length,134217728), (ipc.maximum.response.length,134217728), (ipc.ping.interval,60000), (ipc.server.listen.queue.size,256), (ipc.server.log.slow.rpc,false), (ipc.server.max.connections,0), (ipc.server.reuseaddr,true), (map.sort.class,org.apache.hadoop.util.QuickSort), (mapreduce.am.max-attempts,2), (mapreduce.app-submission.cross-platform,false), (mapreduce.client.completion.pollinterval,5000), (mapreduce.client.libjars.wildcard,true), (mapreduce.client.output.filter,FAILED), (mapreduce.client.progressmonitor.pollinterval,1000), (mapreduce.client.submit.file.replication,10), (mapreduce.cluster.acls.enabled,false), (mapreduce.cluster.local.dir,${hadoop.tmp.dir}/mapred/local), (mapreduce.fileoutputcommitter.algorithm.version,1), (mapreduce.fileoutputcommitter.task.cleanup.enabled,false), (mapreduce.framework.name,local), (mapreduce.ifile.readahead,true), (mapreduce.ifile.readahead.bytes,4194304), (mapreduce.input.fileinputformat.list-status.num-threads,1), (mapreduce.input.fileinputformat.split.minsize,0), (mapreduce.input.lineinputformat.linespermap,1), (mapreduce.job.acl-modify-job, ), (mapreduce.job.acl-view-job, ), (mapreduce.job.cache.limit.max-resources,0), (mapreduce.job.cache.limit.max-resources-mb,0), (mapreduce.job.cache.limit.max-single-resource-mb,0), (mapreduce.job.classloader,false), (mapreduce.job.committer.setup.cleanup.needed,true), (mapreduce.job.complete.cancel.delegation.tokens,*********(redacted)), (mapreduce.job.counters.max,120), (mapreduce.job.dfs.storage.capacity.kill-limit-exceed,false), (mapreduce.job.emit-timeline-data,false), (mapreduce.job.encrypted-intermediate-data,false), (mapreduce.job.encrypted-intermediate-data-key-size-bits,128), (mapreduce.job.encrypted-intermediate-data.buffer.kb,128), (mapreduce.job.end-notification.max.attempts,5), (mapreduce.job.end-notification.max.retry.interval,5000), (mapreduce.job.end-notification.retry.attempts,0), (mapreduce.job.end-notification.retry.interval,1000), (mapreduce.job.finish-when-all-reducers-done,true), (mapreduce.job.hdfs-servers,${fs.defaultFS}), (mapreduce.job.heap.memory-mb.ratio,0.8), (mapreduce.job.local-fs.single-disk-limit.bytes,-1), (mapreduce.job.local-fs.single-disk-limit.check.interval-ms,5000), (mapreduce.job.local-fs.single-disk-limit.check.kill-limit-exceed,true), (mapreduce.job.map.output.collector.class,org.apache.hadoop.mapred.MapTask$MapOutputBuffer), (mapreduce.job.maps,2), (mapreduce.job.max.map,-1), (mapreduce.job.max.split.locations,15), (mapreduce.job.maxtaskfailures.per.tracker,3), (mapreduce.job.queuename,default), (mapreduce.job.reduce.shuffle.consumer.plugin.class,org.apache.hadoop.mapreduce.task.reduce.Shuffle), (mapreduce.job.reduce.slowstart.completedmaps,0.05), (mapreduce.job.reducer.preempt.delay.sec,0), (mapreduce.job.reducer.unconditional-preempt.delay.sec,300), (mapreduce.job.reduces,1), (mapreduce.job.running.map.limit,0), (mapreduce.job.running.reduce.limit,0), (mapreduce.job.sharedcache.mode,disabled), (mapreduce.job.speculative.minimum-allowed-tasks,10), (mapreduce.job.speculative.retry-after-no-speculate,1000), (mapreduce.job.speculative.retry-after-speculate,15000), (mapreduce.job.speculative.slowtaskthreshold,1.0), (mapreduce.job.speculative.speculative-cap-running-tasks,0.1), (mapreduce.job.speculative.speculative-cap-total-tasks,0.01), (mapreduce.job.split.metainfo.maxsize,10000000), (mapreduce.job.token.tracking.ids.enabled,*********(redacted)), (mapreduce.job.ubertask.enable,false), (mapreduce.job.ubertask.maxmaps,9), (mapreduce.job.ubertask.maxreduces,1), (mapreduce.jobhistory.address,0.0.0.0:10020), (mapreduce.jobhistory.admin.acl,*), (mapreduce.jobhistory.admin.address,0.0.0.0:10033), (mapreduce.jobhistory.always-scan-user-dir,false), (mapreduce.jobhistory.cleaner.enable,true), (mapreduce.jobhistory.cleaner.interval-ms,86400000), (mapreduce.jobhistory.client.thread-count,10), (mapreduce.jobhistory.datestring.cache.size,200000), (mapreduce.jobhistory.done-dir,${yarn.app.mapreduce.am.staging-dir}/history/done), (mapreduce.jobhistory.http.policy,HTTP_ONLY), (mapreduce.jobhistory.intermediate-done-dir,${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate), (mapreduce.jobhistory.intermediate-user-done-dir.permissions,770), (mapreduce.jobhistory.jhist.format,binary), (mapreduce.jobhistory.joblist.cache.size,20000), (mapreduce.jobhistory.jobname.limit,50), (mapreduce.jobhistory.keytab,/etc/security/keytab/jhs.service.keytab), (mapreduce.jobhistory.loadedjob.tasks.max,-1), (mapreduce.jobhistory.loadedjobs.cache.size,5), (mapreduce.jobhistory.max-age-ms,604800000), (mapreduce.jobhistory.minicluster.fixed.ports,false), (mapreduce.jobhistory.move.interval-ms,180000), (mapreduce.jobhistory.move.thread-count,3), (mapreduce.jobhistory.principal,jhs/_HOST@REALM.TLD), (mapreduce.jobhistory.recovery.enable,false), (mapreduce.jobhistory.recovery.store.class,org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService), (mapreduce.jobhistory.recovery.store.fs.uri,${hadoop.tmp.dir}/mapred/history/recoverystore), (mapreduce.jobhistory.recovery.store.leveldb.path,${hadoop.tmp.dir}/mapred/history/recoverystore), (mapreduce.jobhistory.webapp.address,0.0.0.0:19888), (mapreduce.jobhistory.webapp.https.address,0.0.0.0:19890), (mapreduce.jobhistory.webapp.rest-csrf.custom-header,X-XSRF-Header), (mapreduce.jobhistory.webapp.rest-csrf.enabled,false), (mapreduce.jobhistory.webapp.rest-csrf.methods-to-ignore,GET,OPTIONS,HEAD), (mapreduce.jobhistory.webapp.xfs-filter.xframe-options,SAMEORIGIN), (mapreduce.jvm.system-properties-to-log,os.name,os.version,java.home,java.runtime.version,java.vendor,java.version,java.vm.name,java.class.path,java.io.tmpdir,user.dir,user.name), (mapreduce.map.cpu.vcores,1), (mapreduce.map.log.level,INFO), (mapreduce.map.maxattempts,4), (mapreduce.map.memory.mb,-1), (mapreduce.map.output.compress,false), (mapreduce.map.output.compress.codec,org.apache.hadoop.io.compress.DefaultCodec), (mapreduce.map.skip.maxrecords,0), (mapreduce.map.skip.proc-count.auto-incr,true), (mapreduce.map.sort.spill.percent,0.80), (mapreduce.map.speculative,true), (mapreduce.output.fileoutputformat.compress,false), (mapreduce.output.fileoutputformat.compress.codec,org.apache.hadoop.io.compress.DefaultCodec), (mapreduce.output.fileoutputformat.compress.type,RECORD), (mapreduce.outputcommitter.factory.scheme.s3a,org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory), (mapreduce.reduce.cpu.vcores,1), (mapreduce.reduce.input.buffer.percent,0.0), (mapreduce.reduce.log.level,INFO), (mapreduce.reduce.markreset.buffer.percent,0.0), (mapreduce.reduce.maxattempts,4), (mapreduce.reduce.memory.mb,-1), (mapreduce.reduce.merge.inmem.threshold,1000), (mapreduce.reduce.shuffle.connect.timeout,180000), (mapreduce.reduce.shuffle.fetch.retry.enabled,${yarn.nodemanager.recovery.enabled}), (mapreduce.reduce.shuffle.fetch.retry.interval-ms,1000), (mapreduce.reduce.shuffle.fetch.retry.timeout-ms,30000), (mapreduce.reduce.shuffle.input.buffer.percent,0.70), (mapreduce.reduce.shuffle.memory.limit.percent,0.25), (mapreduce.reduce.shuffle.merge.percent,0.66), (mapreduce.reduce.shuffle.parallelcopies,5), (mapreduce.reduce.shuffle.read.timeout,180000), (mapreduce.reduce.shuffle.retry-delay.max.ms,60000), (mapreduce.reduce.skip.maxgroups,0), (mapreduce.reduce.skip.proc-count.auto-incr,true), (mapreduce.reduce.speculative,true), (mapreduce.shuffle.connection-keep-alive.enable,false), (mapreduce.shuffle.connection-keep-alive.timeout,5), (mapreduce.shuffle.listen.queue.size,128), (mapreduce.shuffle.max.connections,0), (mapreduce.shuffle.max.threads,0), (mapreduce.shuffle.pathcache.concurrency-level,16), (mapreduce.shuffle.pathcache.expire-after-access-minutes,5), (mapreduce.shuffle.pathcache.max-weight,10485760), (mapreduce.shuffle.port,13562), (mapreduce.shuffle.ssl.enabled,false), (mapreduce.shuffle.ssl.file.buffer.size,65536), (mapreduce.shuffle.transfer.buffer.size,131072), (mapreduce.task.combine.progress.records,10000), (mapreduce.task.exit.timeout,60000), (mapreduce.task.exit.timeout.check-interval-ms,20000), (mapreduce.task.files.preserve.failedtasks,false), (mapreduce.task.io.sort.factor,10), (mapreduce.task.io.sort.mb,100), (mapreduce.task.local-fs.write-limit.bytes,-1), (mapreduce.task.merge.progress.records,10000), (mapreduce.task.profile,false), (mapreduce.task.profile.map.params,${mapreduce.task.profile.params}), (mapreduce.task.profile.maps,0-2), (mapreduce.task.profile.params,-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s), (mapreduce.task.profile.reduce.params,${mapreduce.task.profile.params}), (mapreduce.task.profile.reduces,0-2), (mapreduce.task.skip.start.attempts,2), (mapreduce.task.stuck.timeout-ms,600000), (mapreduce.task.timeout,600000), (mapreduce.task.userlog.limit.kb,0), (net.topology.impl,org.apache.hadoop.net.NetworkTopology), (net.topology.node.switch.mapping.impl,org.apache.hadoop.net.ScriptBasedMapping), (net.topology.script.number.args,100), (nfs.exports.allowed.hosts,* rw), (rpc.metrics.quantile.enable,false), (seq.io.sort.factor,100), (seq.io.sort.mb,100), (tfile.fs.input.buffer.size,262144), (tfile.fs.output.buffer.size,262144), (tfile.io.chunk.size,1048576), (yarn.acl.enable,false), (yarn.acl.reservation-enable,false), (yarn.admin.acl,*), (yarn.am.liveness-monitor.expiry-interval-ms,600000), (yarn.app.attempt.diagnostics.limit.kc,64), (yarn.app.mapreduce.am.command-opts,-Xmx1024m), (yarn.app.mapreduce.am.container.log.backups,0), (yarn.app.mapreduce.am.container.log.limit.kb,0), (yarn.app.mapreduce.am.containerlauncher.threadpool-initial-size,10), (yarn.app.mapreduce.am.hard-kill-timeout-ms,10000), (yarn.app.mapreduce.am.job.committer.cancel-timeout,60000), (yarn.app.mapreduce.am.job.committer.commit-window,10000), (yarn.app.mapreduce.am.job.task.listener.thread-count,30), (yarn.app.mapreduce.am.log.level,INFO), (yarn.app.mapreduce.am.resource.cpu-vcores,1), (yarn.app.mapreduce.am.resource.mb,1536), (yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms,1000), (yarn.app.mapreduce.am.staging-dir,/tmp/hadoop-yarn/staging), (yarn.app.mapreduce.am.staging-dir.erasurecoding.enabled,false), (yarn.app.mapreduce.am.webapp.https.client.auth,false), (yarn.app.mapreduce.am.webapp.https.enabled,false), (yarn.app.mapreduce.client-am.ipc.max-retries,3), (yarn.app.mapreduce.client-am.ipc.max-retries-on-timeouts,3), (yarn.app.mapreduce.client.job.max-retries,3), (yarn.app.mapreduce.client.job.retry-interval,2000), (yarn.app.mapreduce.client.max-retries,3), (yarn.app.mapreduce.shuffle.log.backups,0), (yarn.app.mapreduce.shuffle.log.limit.kb,0), (yarn.app.mapreduce.shuffle.log.separate,true), (yarn.app.mapreduce.task.container.log.backups,0), (yarn.client.application-client-protocol.poll-interval-ms,200), (yarn.client.application-client-protocol.poll-timeout-ms,-1), (yarn.client.failover-no-ha-proxy-provider,org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider), (yarn.client.failover-proxy-provider,org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider), (yarn.client.failover-retries,0), (yarn.client.failover-retries-on-socket-timeouts,0), (yarn.client.load.resource-types.from-server,false), (yarn.client.max-cached-nodemanagers-proxies,0), (yarn.client.nodemanager-client-async.thread-pool-max-size,500), (yarn.client.nodemanager-connect.max-wait-ms,180000), (yarn.client.nodemanager-connect.retry-interval-ms,10000), (yarn.cluster.max-application-priority,0), (yarn.dispatcher.cpu-monitor.samples-per-min,60), (yarn.dispatcher.drain-events.timeout,300000), (yarn.dispatcher.print-events-info.threshold,5000), (yarn.fail-fast,false), (yarn.federation.cache-ttl.secs,300), (yarn.federation.enabled,false), (yarn.federation.registry.base-dir,yarnfederation/), (yarn.federation.state-store.class,org.apache.hadoop.yarn.server.federation.store.impl.MemoryFederationStateStore), (yarn.federation.subcluster-resolver.class,org.apache.hadoop.yarn.server.federation.resolver.DefaultSubClusterResolverImpl), (yarn.http.policy,HTTP_ONLY), (yarn.intermediate-data-encryption.enable,false), (yarn.ipc.rpc.class,org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC), (yarn.is.minicluster,false), (yarn.log-aggregation-enable,false), (yarn.log-aggregation-status.time-out.ms,600000), (yarn.log-aggregation.debug.filesize,104857600), (yarn.log-aggregation.file-controller.TFile.class,org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController), (yarn.log-aggregation.file-formats,TFile), (yarn.log-aggregation.retain-check-interval-seconds,-1), (yarn.log-aggregation.retain-seconds,-1), (yarn.minicluster.control-resource-monitoring,false), (yarn.minicluster.fixed.ports,false), (yarn.minicluster.use-rpc,false), (yarn.minicluster.yarn.nodemanager.resource.memory-mb,4096), (yarn.nm.liveness-monitor.expiry-interval-ms,600000), (yarn.node-attribute.fs-store.impl.class,org.apache.hadoop.yarn.server.resourcemanager.nodelabels.FileSystemNodeAttributeStore), (yarn.node-labels.configuration-type,centralized), (yarn.node-labels.enabled,false), (yarn.node-labels.fs-store.impl.class,org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore), (yarn.nodemanager.address,${yarn.nodemanager.hostname}:0), (yarn.nodemanager.admin-env,MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX), (yarn.nodemanager.amrmproxy.address,0.0.0.0:8049), (yarn.nodemanager.amrmproxy.client.thread-count,25), (yarn.nodemanager.amrmproxy.enabled,false), (yarn.nodemanager.amrmproxy.ha.enable,false), (yarn.nodemanager.amrmproxy.interceptor-class.pipeline,org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor), (yarn.nodemanager.aux-services.manifest.enabled,false), (yarn.nodemanager.aux-services.manifest.reload-ms,0), (yarn.nodemanager.aux-services.mapreduce_shuffle.class,org.apache.hadoop.mapred.ShuffleHandler), (yarn.nodemanager.collector-service.address,${yarn.nodemanager.hostname}:8048), (yarn.nodemanager.collector-service.thread-count,5), (yarn.nodemanager.container-diagnostics-maximum-size,10000), (yarn.nodemanager.container-executor.class,org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor), (yarn.nodemanager.container-executor.exit-code-file.timeout-ms,2000), (yarn.nodemanager.container-localizer.java.opts,-Xmx256m), (yarn.nodemanager.container-localizer.log.level,INFO), (yarn.nodemanager.container-log-monitor.dir-size-limit-bytes,1000000000), (yarn.nodemanager.container-log-monitor.enable,false), (yarn.nodemanager.container-log-monitor.interval-ms,60000), (yarn.nodemanager.container-log-monitor.total-size-limit-bytes,10000000000), (yarn.nodemanager.container-manager.thread-count,20), (yarn.nodemanager.container-metrics.enable,true), (yarn.nodemanager.container-metrics.period-ms,-1), (yarn.nodemanager.container-metrics.unregister-delay-ms,10000), (yarn.nodemanager.container-monitor.enabled,true), (yarn.nodemanager.container-monitor.procfs-tree.smaps-based-rss.enabled,false), (yarn.nodemanager.container-retry-minimum-interval-ms,1000), (yarn.nodemanager.container.stderr.pattern,{*stderr*,*STDERR*}), (yarn.nodemanager.container.stderr.tail.bytes,4096), (yarn.nodemanager.containers-launcher.class,org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher), (yarn.nodemanager.default-container-executor.log-dirs.permissions,710), (yarn.nodemanager.delete.debug-delay-sec,0), (yarn.nodemanager.delete.thread-count,4), (yarn.nodemanager.disk-health-checker.disk-free-space-threshold.enabled,true), (yarn.nodemanager.disk-health-checker.disk-utilization-threshold.enabled,true), (yarn.nodemanager.disk-health-checker.enable,true), (yarn.nodemanager.disk-health-checker.interval-ms,120000), (yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage,90.0), (yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb,0), (yarn.nodemanager.disk-health-checker.min-free-space-per-disk-watermark-high-mb,0), (yarn.nodemanager.disk-health-checker.min-healthy-disks,0.25), (yarn.nodemanager.disk-validator,basic), (yarn.nodemanager.distributed-scheduling.enabled,false), (yarn.nodemanager.elastic-memory-control.enabled,false), (yarn.nodemanager.elastic-memory-control.oom-handler,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.DefaultOOMHandler), (yarn.nodemanager.elastic-memory-control.timeout-sec,5), (yarn.nodemanager.emit-container-events,true), (yarn.nodemanager.env-whitelist,JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ), (yarn.nodemanager.health-checker.interval-ms,600000), (yarn.nodemanager.health-checker.run-before-startup,false), (yarn.nodemanager.health-checker.scripts,script), (yarn.nodemanager.health-checker.timeout-ms,1200000), (yarn.nodemanager.hostname,0.0.0.0), (yarn.nodemanager.keytab,/etc/krb5.keytab), (yarn.nodemanager.linux-container-executor.cgroups.delete-delay-ms,20), (yarn.nodemanager.linux-container-executor.cgroups.delete-timeout-ms,1000), (yarn.nodemanager.linux-container-executor.cgroups.hierarchy,/hadoop-yarn), (yarn.nodemanager.linux-container-executor.cgroups.mount,false), (yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage,false), (yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users,true), (yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user,nobody), (yarn.nodemanager.linux-container-executor.nonsecure-mode.user-pattern,^[_.A-Za-z0-9][-@_.A-Za-z0-9]{0,255}?[$]?$), (yarn.nodemanager.linux-container-executor.resources-handler.class,org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler), (yarn.nodemanager.local-cache.max-files-per-directory,8192), (yarn.nodemanager.local-dirs,${hadoop.tmp.dir}/nm-local-dir), (yarn.nodemanager.localizer.address,${yarn.nodemanager.hostname}:8040), (yarn.nodemanager.localizer.cache.cleanup.interval-ms,600000), (yarn.nodemanager.localizer.cache.target-size-mb,10240), (yarn.nodemanager.localizer.client.thread-count,5), (yarn.nodemanager.localizer.fetch.thread-count,4), (yarn.nodemanager.log-aggregation.compression-type,none), (yarn.nodemanager.log-aggregation.num-log-files-per-app,30), (yarn.nodemanager.log-aggregation.policy.class,org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AllContainerLogAggregationPolicy), (yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds,-1), (yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds.min,3600), (yarn.nodemanager.log-container-debug-info.enabled,true), (yarn.nodemanager.log-dirs,${yarn.log.dir}/userlogs), (yarn.nodemanager.log.deletion-threads-count,4), (yarn.nodemanager.log.retain-seconds,10800), (yarn.nodemanager.logaggregation.threadpool-size-max,100), (yarn.nodemanager.node-attributes.provider.fetch-interval-ms,600000), (yarn.nodemanager.node-attributes.provider.fetch-timeout-ms,1200000), (yarn.nodemanager.node-attributes.resync-interval-ms,120000), (yarn.nodemanager.node-labels.provider.fetch-interval-ms,600000), (yarn.nodemanager.node-labels.provider.fetch-timeout-ms,1200000), (yarn.nodemanager.node-labels.resync-interval-ms,120000), (yarn.nodemanager.numa-awareness.enabled,false), (yarn.nodemanager.numa-awareness.numactl.cmd,/usr/bin/numactl), (yarn.nodemanager.numa-awareness.read-topology,false), (yarn.nodemanager.opportunistic-containers-max-queue-length,0), (yarn.nodemanager.opportunistic-containers-use-pause-for-preemption,false), (yarn.nodemanager.pluggable-device-framework.enabled,false), (yarn.nodemanager.pmem-check-enabled,true), (yarn.nodemanager.process-kill-wait.ms,5000), (yarn.nodemanager.recovery.compaction-interval-secs,3600), (yarn.nodemanager.recovery.dir,${hadoop.tmp.dir}/yarn-nm-recovery), (yarn.nodemanager.recovery.enabled,false), (yarn.nodemanager.recovery.supervised,false), (yarn.nodemanager.remote-app-log-dir,/tmp/logs), (yarn.nodemanager.remote-app-log-dir-include-older,true), (yarn.nodemanager.remote-app-log-dir-suffix,logs), (yarn.nodemanager.resource-monitor.interval-ms,3000), (yarn.nodemanager.resource-plugins.fpga.allowed-fpga-devices,auto), (yarn.nodemanager.resource-plugins.fpga.vendor-plugin.class,org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin), (yarn.nodemanager.resource-plugins.gpu.allowed-gpu-devices,auto), (yarn.nodemanager.resource-plugins.gpu.docker-plugin,nvidia-docker-v1), (yarn.nodemanager.resource-plugins.gpu.docker-plugin.nvidia-docker-v1.endpoint,http://localhost:3476/v1.0/docker/cli), (yarn.nodemanager.resource.count-logical-processors-as-cores,false), (yarn.nodemanager.resource.cpu-vcores,-1), (yarn.nodemanager.resource.detect-hardware-capabilities,false), (yarn.nodemanager.resource.memory-mb,-1), (yarn.nodemanager.resource.memory.cgroups.soft-limit-percentage,90.0), (yarn.nodemanager.resource.memory.cgroups.swappiness,0), (yarn.nodemanager.resource.memory.enabled,false), (yarn.nodemanager.resource.memory.enforced,true), (yarn.nodemanager.resource.pcores-vcores-multiplier,1.0), (yarn.nodemanager.resource.percentage-physical-cpu-limit,100), (yarn.nodemanager.resource.system-reserved-memory-mb,-1), (yarn.nodemanager.resourcemanager.minimum.version,NONE), (yarn.nodemanager.runtime.linux.allowed-runtimes,default), (yarn.nodemanager.runtime.linux.docker.allowed-container-networks,host,none,bridge), (yarn.nodemanager.runtime.linux.docker.allowed-container-runtimes,runc), (yarn.nodemanager.runtime.linux.docker.capabilities,CHOWN,DAC_OVERRIDE,FSETID,FOWNER,MKNOD,NET_RAW,SETGID,SETUID,SETFCAP,SETPCAP,NET_BIND_SERVICE,SYS_CHROOT,KILL,AUDIT_WRITE), (yarn.nodemanager.runtime.linux.docker.default-container-network,host), (yarn.nodemanager.runtime.linux.docker.delayed-removal.allowed,false), (yarn.nodemanager.runtime.linux.docker.enable-userremapping.allowed,true), (yarn.nodemanager.runtime.linux.docker.host-pid-namespace.allowed,false), (yarn.nodemanager.runtime.linux.docker.image-update,false), (yarn.nodemanager.runtime.linux.docker.privileged-containers.allowed,false), (yarn.nodemanager.runtime.linux.docker.stop.grace-period,10), (yarn.nodemanager.runtime.linux.docker.userremapping-gid-threshold,1), (yarn.nodemanager.runtime.linux.docker.userremapping-uid-threshold,1), (yarn.nodemanager.runtime.linux.runc.allowed-container-networks,host,none,bridge), (yarn.nodemanager.runtime.linux.runc.allowed-container-runtimes,runc), (yarn.nodemanager.runtime.linux.runc.hdfs-manifest-to-resources-plugin.stat-cache-size,500), (yarn.nodemanager.runtime.linux.runc.hdfs-manifest-to-resources-plugin.stat-cache-timeout-interval-secs,360), (yarn.nodemanager.runtime.linux.runc.host-pid-namespace.allowed,false), (yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.ImageTagToManifestPlugin), (yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin.cache-refresh-interval-secs,60), (yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin.hdfs-hash-file,/runc-root/image-tag-to-hash), (yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin.num-manifests-to-cache,10), (yarn.nodemanager.runtime.linux.runc.image-toplevel-dir,/runc-root), (yarn.nodemanager.runtime.linux.runc.layer-mounts-interval-secs,600), (yarn.nodemanager.runtime.linux.runc.layer-mounts-to-keep,100), (yarn.nodemanager.runtime.linux.runc.manifest-to-resources-plugin,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.HdfsManifestToResourcesPlugin), (yarn.nodemanager.runtime.linux.runc.privileged-containers.allowed,false), (yarn.nodemanager.runtime.linux.sandbox-mode,disabled), (yarn.nodemanager.runtime.linux.sandbox-mode.local-dirs.permissions,read), (yarn.nodemanager.sleep-delay-before-sigkill.ms,250), (yarn.nodemanager.vmem-check-enabled,true), (yarn.nodemanager.vmem-pmem-ratio,2.1), (yarn.nodemanager.webapp.address,${yarn.nodemanager.hostname}:8042), (yarn.nodemanager.webapp.cross-origin.enabled,false), (yarn.nodemanager.webapp.https.address,0.0.0.0:8044), (yarn.nodemanager.webapp.rest-csrf.custom-header,X-XSRF-Header), (yarn.nodemanager.webapp.rest-csrf.enabled,false), (yarn.nodemanager.webapp.rest-csrf.methods-to-ignore,GET,OPTIONS,HEAD), (yarn.nodemanager.webapp.xfs-filter.xframe-options,SAMEORIGIN), (yarn.nodemanager.windows-container.cpu-limit.enabled,false), (yarn.nodemanager.windows-container.memory-limit.enabled,false), (yarn.registry.class,org.apache.hadoop.registry.client.impl.FSRegistryOperationsService), (yarn.resourcemanager.activities-manager.app-activities.max-queue-length,100), (yarn.resourcemanager.activities-manager.app-activities.ttl-ms,600000), (yarn.resourcemanager.activities-manager.cleanup-interval-ms,5000), (yarn.resourcemanager.activities-manager.scheduler-activities.ttl-ms,600000), (yarn.resourcemanager.address,${yarn.resourcemanager.hostname}:8032), (yarn.resourcemanager.admin.address,${yarn.resourcemanager.hostname}:8033), (yarn.resourcemanager.admin.client.thread-count,1), (yarn.resourcemanager.am-rm-tokens.master-key-rolling-interval-secs,*********(redacted)), (yarn.resourcemanager.am.max-attempts,2), (yarn.resourcemanager.amlauncher.thread-count,50), (yarn.resourcemanager.application-https.policy,NONE), (yarn.resourcemanager.application-tag-based-placement.enable,false), (yarn.resourcemanager.application-timeouts.monitor.interval-ms,3000), (yarn.resourcemanager.application.max-tag.length,100), (yarn.resourcemanager.application.max-tags,10), (yarn.resourcemanager.auto-update.containers,false), (yarn.resourcemanager.client.thread-count,50), (yarn.resourcemanager.configuration.file-system-based-store,/yarn/conf), (yarn.resourcemanager.configuration.provider-class,org.apache.hadoop.yarn.LocalConfigurationProvider), (yarn.resourcemanager.connect.max-wait.ms,900000), (yarn.resourcemanager.connect.retry-interval.ms,30000), (yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs,*********(redacted)), (yarn.resourcemanager.container.liveness-monitor.interval-ms,600000), (yarn.resourcemanager.decommissioning-nodes-watcher.poll-interval-secs,20), (yarn.resourcemanager.delayed.delegation-token.removal-interval-ms,*********(redacted)), (yarn.resourcemanager.delegation-token-renewer.thread-count,*********(redacted)), (yarn.resourcemanager.delegation-token-renewer.thread-retry-interval,*********(redacted)), (yarn.resourcemanager.delegation-token-renewer.thread-retry-max-attempts,*********(redacted)), (yarn.resourcemanager.delegation-token-renewer.thread-timeout,*********(redacted)), (yarn.resourcemanager.delegation-token.always-cancel,*********(redacted)), (yarn.resourcemanager.delegation-token.max-conf-size-bytes,*********(redacted)), (yarn.resourcemanager.delegation.key.update-interval,86400000), (yarn.resourcemanager.delegation.token.max-lifetime,*********(redacted)), (yarn.resourcemanager.delegation.token.renew-interval,*********(redacted)), (yarn.resourcemanager.epoch.range,0), (yarn.resourcemanager.fail-fast,${yarn.fail-fast}), (yarn.resourcemanager.fs.state-store.num-retries,0), (yarn.resourcemanager.fs.state-store.retry-interval-ms,1000), (yarn.resourcemanager.fs.state-store.uri,${hadoop.tmp.dir}/yarn/system/rmstore), (yarn.resourcemanager.ha.automatic-failover.embedded,true), (yarn.resourcemanager.ha.automatic-failover.enabled,true), (yarn.resourcemanager.ha.automatic-failover.zk-base-path,/yarn-leader-election), (yarn.resourcemanager.ha.enabled,false), (yarn.resourcemanager.history-writer.multi-threaded-dispatcher.pool-size,10), (yarn.resourcemanager.hostname,0.0.0.0), (yarn.resourcemanager.keytab,/etc/krb5.keytab), (yarn.resourcemanager.leveldb-state-store.compaction-interval-secs,3600), (yarn.resourcemanager.leveldb-state-store.path,${hadoop.tmp.dir}/yarn/system/rmstore), (yarn.resourcemanager.max-completed-applications,1000), (yarn.resourcemanager.max-log-aggregation-diagnostics-in-memory,10), (yarn.resourcemanager.metrics.runtime.buckets,60,300,1440), (yarn.resourcemanager.nm-container-queuing.load-comparator,QUEUE_LENGTH), (yarn.resourcemanager.nm-container-queuing.max-queue-length,15), (yarn.resourcemanager.nm-container-queuing.max-queue-wait-time-ms,100), (yarn.resourcemanager.nm-container-queuing.min-queue-length,5), (yarn.resourcemanager.nm-container-queuing.min-queue-wait-time-ms,10), (yarn.resourcemanager.nm-container-queuing.queue-limit-stdev,1.0f), (yarn.resourcemanager.nm-container-queuing.sorting-nodes-interval-ms,1000), (yarn.resourcemanager.nm-tokens.master-key-rolling-interval-secs,*********(redacted)), (yarn.resourcemanager.node-ip-cache.expiry-interval-secs,-1), (yarn.resourcemanager.node-labels.provider.fetch-interval-ms,1800000), (yarn.resourcemanager.node-removal-untracked.timeout-ms,60000), (yarn.resourcemanager.nodemanager-connect-retries,10), (yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs,3600), (yarn.resourcemanager.nodemanager.minimum.version,NONE), (yarn.resourcemanager.nodemanagers.heartbeat-interval-max-ms,1000), (yarn.resourcemanager.nodemanagers.heartbeat-interval-min-ms,1000), (yarn.resourcemanager.nodemanagers.heartbeat-interval-ms,1000), (yarn.resourcemanager.nodemanagers.heartbeat-interval-scaling-enable,false), (yarn.resourcemanager.nodemanagers.heartbeat-interval-slowdown-factor,1.0), (yarn.resourcemanager.nodemanagers.heartbeat-interval-speedup-factor,1.0), (yarn.resourcemanager.opportunistic-container-allocation.enabled,false), (yarn.resourcemanager.opportunistic-container-allocation.nodes-used,10), (yarn.resourcemanager.opportunistic.max.container-allocation.per.am.heartbeat,-1), (yarn.resourcemanager.placement-constraints.algorithm.class,org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.algorithm.DefaultPlacementAlgorithm), (yarn.resourcemanager.placement-constraints.algorithm.iterator,SERIAL), (yarn.resourcemanager.placement-constraints.algorithm.pool-size,1), (yarn.resourcemanager.placement-constraints.handler,disabled), (yarn.resourcemanager.placement-constraints.retry-attempts,3), (yarn.resourcemanager.placement-constraints.scheduler.pool-size,1), (yarn.resourcemanager.proxy-user-privileges.enabled,false), (yarn.resourcemanager.recovery.enabled,false), (yarn.resourcemanager.reservation-system.enable,false), (yarn.resourcemanager.reservation-system.planfollower.time-step,1000), (yarn.resourcemanager.resource-profiles.enabled,false), (yarn.resourcemanager.resource-profiles.source-file,resource-profiles.json), (yarn.resourcemanager.resource-tracker.address,${yarn.resourcemanager.hostname}:8031), (yarn.resourcemanager.resource-tracker.client.thread-count,50), (yarn.resourcemanager.resource-tracker.nm.ip-hostname-check,false), (yarn.resourcemanager.rm.container-allocation.expiry-interval-ms,600000), (yarn.resourcemanager.scheduler.address,${yarn.resourcemanager.hostname}:8030), (yarn.resourcemanager.scheduler.class,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler), (yarn.resourcemanager.scheduler.client.thread-count,50), (yarn.resourcemanager.scheduler.monitor.enable,false), (yarn.resourcemanager.scheduler.monitor.policies,org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy), (yarn.resourcemanager.state-store.max-completed-applications,${yarn.resourcemanager.max-completed-applications}), (yarn.resourcemanager.store.class,org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore), (yarn.resourcemanager.submission-preprocessor.enabled,false), (yarn.resourcemanager.submission-preprocessor.file-refresh-interval-ms,60000), (yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size,10), (yarn.resourcemanager.system-metrics-publisher.enabled,false), (yarn.resourcemanager.webapp.address,${yarn.resourcemanager.hostname}:8088), (yarn.resourcemanager.webapp.cross-origin.enabled,false), (yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled,*********(redacted)), (yarn.resourcemanager.webapp.https.address,${yarn.resourcemanager.hostname}:8090), (yarn.resourcemanager.webapp.rest-csrf.custom-header,X-XSRF-Header), (yarn.resourcemanager.webapp.rest-csrf.enabled,false), (yarn.resourcemanager.webapp.rest-csrf.methods-to-ignore,GET,OPTIONS,HEAD), (yarn.resourcemanager.webapp.ui-actions.enabled,true), (yarn.resourcemanager.webapp.xfs-filter.xframe-options,SAMEORIGIN), (yarn.resourcemanager.work-preserving-recovery.enabled,true), (yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms,10000), (yarn.resourcemanager.zk-appid-node.split-index,0), (yarn.resourcemanager.zk-delegation-token-node.split-index,*********(redacted)), (yarn.resourcemanager.zk-max-znode-size.bytes,1048576), (yarn.resourcemanager.zk-state-store.parent-path,/rmstore), (yarn.rm.system-metrics-publisher.emit-container-events,false), (yarn.router.clientrm.interceptor-class.pipeline,org.apache.hadoop.yarn.server.router.clientrm.DefaultClientRequestInterceptor), (yarn.router.interceptor.user.threadpool-size,5), (yarn.router.pipeline.cache-max-size,25), (yarn.router.rmadmin.interceptor-class.pipeline,org.apache.hadoop.yarn.server.router.rmadmin.DefaultRMAdminRequestInterceptor), (yarn.router.webapp.address,0.0.0.0:8089), (yarn.router.webapp.https.address,0.0.0.0:8091), (yarn.router.webapp.interceptor-class.pipeline,org.apache.hadoop.yarn.server.router.webapp.DefaultRequestInterceptorREST), (yarn.scheduler.configuration.fs.path,file://${hadoop.tmp.dir}/yarn/system/schedconf), (yarn.scheduler.configuration.leveldb-store.compaction-interval-secs,86400), (yarn.scheduler.configuration.leveldb-store.path,${hadoop.tmp.dir}/yarn/system/confstore), (yarn.scheduler.configuration.max.version,100), (yarn.scheduler.configuration.mutation.acl-policy.class,org.apache.hadoop.yarn.server.resourcemanager.scheduler.DefaultConfigurationMutationACLPolicy), (yarn.scheduler.configuration.store.class,file), (yarn.scheduler.configuration.store.max-logs,1000), (yarn.scheduler.configuration.zk-store.parent-path,/confstore), (yarn.scheduler.include-port-in-node-name,false), (yarn.scheduler.maximum-allocation-mb,8192), (yarn.scheduler.maximum-allocation-vcores,4), (yarn.scheduler.minimum-allocation-mb,1024), (yarn.scheduler.minimum-allocation-vcores,1), (yarn.scheduler.queue-placement-rules,user-group), (yarn.sharedcache.admin.address,0.0.0.0:8047), (yarn.sharedcache.admin.thread-count,1), (yarn.sharedcache.app-checker.class,org.apache.hadoop.yarn.server.sharedcachemanager.RemoteAppChecker), (yarn.sharedcache.checksum.algo.impl,org.apache.hadoop.yarn.sharedcache.ChecksumSHA256Impl), (yarn.sharedcache.cleaner.initial-delay-mins,10), (yarn.sharedcache.cleaner.period-mins,1440), (yarn.sharedcache.cleaner.resource-sleep-ms,0), (yarn.sharedcache.client-server.address,0.0.0.0:8045), (yarn.sharedcache.client-server.thread-count,50), (yarn.sharedcache.enabled,false), (yarn.sharedcache.nested-level,3), (yarn.sharedcache.nm.uploader.replication.factor,10), (yarn.sharedcache.nm.uploader.thread-count,20), (yarn.sharedcache.root-dir,/sharedcache), (yarn.sharedcache.store.class,org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore), (yarn.sharedcache.store.in-memory.check-period-mins,720), (yarn.sharedcache.store.in-memory.initial-delay-mins,10), (yarn.sharedcache.store.in-memory.staleness-period-mins,10080), (yarn.sharedcache.uploader.server.address,0.0.0.0:8046), (yarn.sharedcache.uploader.server.thread-count,50), (yarn.sharedcache.webapp.address,0.0.0.0:8788), (yarn.system-metrics-publisher.enabled,false), (yarn.timeline-service.address,${yarn.timeline-service.hostname}:10200), (yarn.timeline-service.app-aggregation-interval-secs,15), (yarn.timeline-service.app-collector.linger-period.ms,60000), (yarn.timeline-service.client.best-effort,false), (yarn.timeline-service.client.drain-entities.timeout.ms,2000), (yarn.timeline-service.client.fd-clean-interval-secs,60), (yarn.timeline-service.client.fd-flush-interval-secs,10), (yarn.timeline-service.client.fd-retain-secs,300), (yarn.timeline-service.client.internal-timers-ttl-secs,420), (yarn.timeline-service.client.max-retries,30), (yarn.timeline-service.client.retry-interval-ms,1000), (yarn.timeline-service.enabled,false), (yarn.timeline-service.entity-group-fs-store.active-dir,/tmp/entity-file-history/active), (yarn.timeline-service.entity-group-fs-store.app-cache-size,10), (yarn.timeline-service.entity-group-fs-store.cache-store-class,org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore), (yarn.timeline-service.entity-group-fs-store.cleaner-interval-seconds,3600), (yarn.timeline-service.entity-group-fs-store.done-dir,/tmp/entity-file-history/done/), (yarn.timeline-service.entity-group-fs-store.leveldb-cache-read-cache-size,10485760), (yarn.timeline-service.entity-group-fs-store.retain-seconds,604800), (yarn.timeline-service.entity-group-fs-store.scan-interval-seconds,60), (yarn.timeline-service.entity-group-fs-store.summary-store,org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore), (yarn.timeline-service.entity-group-fs-store.with-user-dir,false), (yarn.timeline-service.flowname.max-size,0), (yarn.timeline-service.generic-application-history.max-applications,10000), (yarn.timeline-service.handler-thread-count,10), (yarn.timeline-service.hbase-schema.prefix,prod.), (yarn.timeline-service.hbase.coprocessor.app-final-value-retention-milliseconds,259200000), (yarn.timeline-service.hbase.coprocessor.jar.hdfs.location,/hbase/coprocessor/hadoop-yarn-server-timelineservice.jar), (yarn.timeline-service.hostname,0.0.0.0), (yarn.timeline-service.http-authentication.simple.anonymous.allowed,true), (yarn.timeline-service.http-authentication.type,simple), (yarn.timeline-service.http-cross-origin.enabled,false), (yarn.timeline-service.keytab,/etc/krb5.keytab), (yarn.timeline-service.leveldb-state-store.path,${hadoop.tmp.dir}/yarn/timeline), (yarn.timeline-service.leveldb-timeline-store.path,${hadoop.tmp.dir}/yarn/timeline), (yarn.timeline-service.leveldb-timeline-store.read-cache-size,104857600), (yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size,10000), (yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size,10000), (yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms,300000), (yarn.timeline-service.reader.class,org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineReaderImpl), (yarn.timeline-service.reader.webapp.address,${yarn.timeline-service.webapp.address}), (yarn.timeline-service.reader.webapp.https.address,${yarn.timeline-service.webapp.https.address}), (yarn.timeline-service.recovery.enabled,false), (yarn.timeline-service.state-store-class,org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore), (yarn.timeline-service.store-class,org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore), (yarn.timeline-service.timeline-client.number-of-async-entities-to-merge,10), (yarn.timeline-service.ttl-enable,true), (yarn.timeline-service.ttl-ms,604800000), (yarn.timeline-service.version,1.0f), (yarn.timeline-service.webapp.address,${yarn.timeline-service.hostname}:8188), (yarn.timeline-service.webapp.https.address,${yarn.timeline-service.hostname}:8190), (yarn.timeline-service.webapp.rest-csrf.custom-header,X-XSRF-Header), (yarn.timeline-service.webapp.rest-csrf.enabled,false), (yarn.timeline-service.webapp.rest-csrf.methods-to-ignore,GET,OPTIONS,HEAD), (yarn.timeline-service.webapp.xfs-filter.xframe-options,SAMEORIGIN), (yarn.timeline-service.writer.async.queue.capacity,100), (yarn.timeline-service.writer.class,org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineWriterImpl), (yarn.timeline-service.writer.flush-interval-seconds,60), (yarn.webapp.api-service.enable,false), (yarn.webapp.enable-rest-app-submissions,true), (yarn.webapp.filter-entity-list-by-user,false), (yarn.webapp.filter-invalid-xml-chars,false), (yarn.webapp.ui2.enable,false), (yarn.webapp.xfs-filter.enabled,true), (yarn.workflow-id.tag-prefix,workflowid:)), System Properties -> Vector((SPARK_SUBMIT,true), (awt.toolkit,sun.awt.X11.XToolkit), (file.encoding,UTF-8), (file.separator,/), (java.awt.graphicsenv,sun.awt.X11GraphicsEnvironment), (java.awt.printerjob,sun.print.PSPrinterJob), (java.class.version,55.0), (java.home,/usr/lib/jvm/java-11-openjdk-amd64), (java.io.tmpdir,/tmp), (java.library.path,/usr/java/packages/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib), (java.runtime.name,OpenJDK Runtime Environment), (java.runtime.version,11.0.12+7-post-Debian-2deb10u1), (java.specification.name,Java Platform API Specification), (java.specification.vendor,Oracle Corporation), (java.specification.version,11), (java.vendor,Debian), (java.vendor.url,https://tracker.debian.org/openjdk-11), (java.vendor.url.bug,https://bugs.debian.org/openjdk-11), (java.version,11.0.12), (java.version.date,2021-07-20), (java.vm.compressedOopsMode,32-bit), (java.vm.info,mixed mode, sharing), (java.vm.name,OpenJDK 64-Bit Server VM), (java.vm.specification.name,Java Virtual Machine Specification), (java.vm.specification.vendor,Oracle Corporation), (java.vm.specification.version,11), (java.vm.vendor,Debian), (java.vm.version,11.0.12+7-post-Debian-2deb10u1), (jdk.debug,release), (jetty.git.hash,526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8), (line.separator,
[2021-12-18 10:22:05,937] {spark_submit.py:514} INFO - ), (os.arch,amd64), (os.name,Linux), (os.version,5.10.76-linuxkit), (path.separator,:), (sun.arch.data.model,64), (sun.boot.library.path,/usr/lib/jvm/java-11-openjdk-amd64/lib), (sun.cpu.endian,little), (sun.cpu.isalist,), (sun.io.unicode.encoding,UnicodeLittle), (sun.java.command,org.apache.spark.deploy.SparkSubmit --master spark://spark:7077 --conf spark.master=spark://spark:7077 --name scrap data --verbose /opt/***/jobs/get_ads_per_county.py dublin), (sun.java.launcher,SUN_STANDARD), (sun.jnu.encoding,UTF-8), (sun.management.compiler,HotSpot 64-Bit Tiered Compilers), (sun.nio.ch.bugLevel,), (sun.os.patch.level,unknown), (user.dir,/opt/***), (user.home,/home/***), (user.language,en), (user.name,***), (user.timezone,Etc/UTC)), JVM Information -> List((Java Home,/usr/lib/jvm/java-11-openjdk-amd64), (Java Version,11.0.12 (Debian)), (Scala Version,version 2.12.15)))) by listener AppStatusListener took 6.6917684s.
[2021-12-18 10:22:06,988] {spark_submit.py:514} INFO - /home/***/.local/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.
[2021-12-18 10:22:07,004] {spark_submit.py:514} INFO - FutureWarning
[2021-12-18 10:22:21,493] {spark_submit.py:514} INFO - 21/12/18 10:22:21 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2021-12-18 10:22:22,548] {spark_submit.py:514} INFO - 21/12/18 10:22:22 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2021-12-18 10:22:49,881] {spark_submit.py:514} INFO - running...
[2021-12-18 10:22:54,314] {spark_submit.py:514} INFO - running...
[2021-12-18 10:22:58,186] {spark_submit.py:514} INFO - running...
[2021-12-18 10:23:00,161] {spark_submit.py:514} INFO - running...
[2021-12-18 10:23:04,032] {spark_submit.py:514} INFO - running...
[2021-12-18 10:23:07,054] {spark_submit.py:514} INFO - running...
[2021-12-18 10:23:10,680] {spark_submit.py:514} INFO - running...
[2021-12-18 10:23:11,503] {spark_submit.py:514} INFO - running...
[2021-12-18 10:23:14,559] {spark_submit.py:514} INFO - running...
[2021-12-18 10:23:17,958] {spark_submit.py:514} INFO - running...
[2021-12-18 10:23:22,067] {spark_submit.py:514} INFO - running...
[2021-12-18 10:23:24,936] {spark_submit.py:514} INFO - running...
[2021-12-18 10:23:26,806] {spark_submit.py:514} INFO - running...
[2021-12-18 10:23:27,943] {spark_submit.py:514} INFO - running...
[2021-12-18 10:23:29,555] {spark_submit.py:514} INFO - running...
[2021-12-18 10:23:30,344] {spark_submit.py:514} INFO - running...
[2021-12-18 10:23:32,265] {spark_submit.py:514} INFO - running...
[2021-12-18 10:23:34,533] {spark_submit.py:514} INFO - running...
[2021-12-18 10:23:38,802] {spark_submit.py:514} INFO - running...
[2021-12-18 10:23:44,152] {spark_submit.py:514} INFO - running...
[2021-12-18 10:23:47,035] {spark_submit.py:514} INFO - running...
[2021-12-18 10:23:48,493] {spark_submit.py:514} INFO - running...
[2021-12-18 10:23:53,575] {spark_submit.py:514} INFO - running...
[2021-12-18 10:23:55,112] {spark_submit.py:514} INFO - running...
[2021-12-18 10:23:55,114] {spark_submit.py:514} INFO - running...
[2021-12-18 10:23:57,190] {spark_submit.py:514} INFO - running...
[2021-12-18 10:23:58,962] {spark_submit.py:514} INFO - running...
[2021-12-18 10:24:00,416] {spark_submit.py:514} INFO - running...
[2021-12-18 10:24:01,707] {spark_submit.py:514} INFO - running...
[2021-12-18 10:24:03,705] {spark_submit.py:514} INFO - running...
[2021-12-18 10:24:06,540] {spark_submit.py:514} INFO - running...
[2021-12-18 10:24:09,509] {spark_submit.py:514} INFO - running...
[2021-12-18 10:24:12,587] {spark_submit.py:514} INFO - running...
[2021-12-18 10:24:13,586] {spark_submit.py:514} INFO - running...
[2021-12-18 10:24:15,101] {spark_submit.py:514} INFO - running...
[2021-12-18 10:24:17,120] {spark_submit.py:514} INFO - running...
[2021-12-18 10:24:18,000] {spark_submit.py:514} INFO - 21/12/18 10:24:17 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20211218102123-0004/0 on worker-20211218101855-172.19.0.6-35967 (172.19.0.6:35967) with 6 core(s)
[2021-12-18 10:24:18,032] {spark_submit.py:514} INFO - 21/12/18 10:24:18 INFO StandaloneSchedulerBackend: Granted executor ID app-20211218102123-0004/0 on hostPort 172.19.0.6:35967 with 6 core(s), 1024.0 MiB RAM
[2021-12-18 10:24:18,049] {spark_submit.py:514} INFO - 21/12/18 10:24:18 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20211218102123-0004/1 on worker-20211218101855-172.19.0.5-42491 (172.19.0.5:42491) with 6 core(s)
[2021-12-18 10:24:18,063] {spark_submit.py:514} INFO - 21/12/18 10:24:18 INFO StandaloneSchedulerBackend: Granted executor ID app-20211218102123-0004/1 on hostPort 172.19.0.5:42491 with 6 core(s), 1024.0 MiB RAM
[2021-12-18 10:24:18,187] {spark_submit.py:514} INFO - 21/12/18 10:24:18 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20211218102123-0004/0 is now RUNNING
[2021-12-18 10:24:18,226] {spark_submit.py:514} INFO - 21/12/18 10:24:18 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20211218102123-0004/1 is now RUNNING
[2021-12-18 10:24:20,854] {spark_submit.py:514} INFO - running...
[2021-12-18 10:24:23,338] {spark_submit.py:514} INFO - running...
[2021-12-18 10:24:24,375] {spark_submit.py:514} INFO - running...
[2021-12-18 10:24:25,574] {spark_submit.py:514} INFO - running...
[2021-12-18 10:24:31,815] {spark_submit.py:514} INFO - 21/12/18 10:24:31 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.19.0.6:55486) with ID 0,  ResourceProfileId 0
[2021-12-18 10:24:31,996] {spark_submit.py:514} INFO - running...
[2021-12-18 10:24:32,213] {spark_submit.py:514} INFO - 21/12/18 10:24:32 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.19.0.5:49842) with ID 1,  ResourceProfileId 0
[2021-12-18 10:24:32,848] {spark_submit.py:514} INFO - 21/12/18 10:24:32 INFO BlockManagerMasterEndpoint: Registering block manager 172.19.0.6:34231 with 366.3 MiB RAM, BlockManagerId(0, 172.19.0.6, 34231, None)
[2021-12-18 10:24:33,142] {spark_submit.py:514} INFO - 21/12/18 10:24:33 INFO BlockManagerMasterEndpoint: Registering block manager 172.19.0.5:41791 with 366.3 MiB RAM, BlockManagerId(1, 172.19.0.5, 41791, None)
[2021-12-18 10:24:33,922] {spark_submit.py:514} INFO - running...
[2021-12-18 10:24:35,372] {spark_submit.py:514} INFO - running...
[2021-12-18 10:24:38,408] {spark_submit.py:514} INFO - running...
[2021-12-18 10:24:39,291] {spark_submit.py:514} INFO - running...
[2021-12-18 10:24:40,598] {spark_submit.py:514} INFO - running...
[2021-12-18 10:24:43,456] {spark_submit.py:514} INFO - running...
[2021-12-18 10:24:44,645] {spark_submit.py:514} INFO - running...
[2021-12-18 10:24:48,217] {spark_submit.py:514} INFO - running...
[2021-12-18 10:24:50,087] {spark_submit.py:514} INFO - running...
[2021-12-18 10:24:52,409] {spark_submit.py:514} INFO - running...
[2021-12-18 10:24:54,096] {spark_submit.py:514} INFO - running...
[2021-12-18 10:24:56,413] {spark_submit.py:514} INFO - running...
[2021-12-18 10:24:59,694] {spark_submit.py:514} INFO - running...
[2021-12-18 10:25:01,837] {spark_submit.py:514} INFO - running...
[2021-12-18 10:25:03,743] {spark_submit.py:514} INFO - running...
[2021-12-18 10:25:07,698] {spark_submit.py:514} INFO - running...
[2021-12-18 10:25:09,996] {spark_submit.py:514} INFO - running...
[2021-12-18 10:25:12,887] {spark_submit.py:514} INFO - running...
[2021-12-18 10:25:14,921] {spark_submit.py:514} INFO - running...
[2021-12-18 10:25:17,729] {spark_submit.py:514} INFO - running...
[2021-12-18 10:25:19,964] {spark_submit.py:514} INFO - running...
[2021-12-18 10:25:21,715] {spark_submit.py:514} INFO - running...
[2021-12-18 10:25:25,214] {spark_submit.py:514} INFO - running...
[2021-12-18 10:25:27,896] {spark_submit.py:514} INFO - running...
[2021-12-18 10:25:30,547] {spark_submit.py:514} INFO - running...
[2021-12-18 10:25:35,412] {spark_submit.py:514} INFO - running...
[2021-12-18 10:25:36,538] {spark_submit.py:514} INFO - running...
[2021-12-18 10:25:38,066] {spark_submit.py:514} INFO - running...
[2021-12-18 10:25:39,942] {spark_submit.py:514} INFO - running...
[2021-12-18 10:25:41,231] {spark_submit.py:514} INFO - running...
[2021-12-18 10:25:42,399] {spark_submit.py:514} INFO - running...
[2021-12-18 10:25:43,554] {spark_submit.py:514} INFO - running...
[2021-12-18 10:25:44,966] {spark_submit.py:514} INFO - running...
[2021-12-18 10:25:46,159] {spark_submit.py:514} INFO - running...
[2021-12-18 10:25:47,723] {spark_submit.py:514} INFO - running...
[2021-12-18 10:25:48,895] {spark_submit.py:514} INFO - running...
[2021-12-18 10:25:49,894] {spark_submit.py:514} INFO - running...
[2021-12-18 10:25:51,057] {spark_submit.py:514} INFO - running...
[2021-12-18 10:25:52,145] {spark_submit.py:514} INFO - running...
[2021-12-18 10:25:55,052] {spark_submit.py:514} INFO - running...
[2021-12-18 10:25:56,529] {spark_submit.py:514} INFO - running...
[2021-12-18 10:25:57,871] {spark_submit.py:514} INFO - running...
[2021-12-18 10:25:59,350] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:00,906] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:01,731] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:02,597] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:03,588] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:05,347] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:07,456] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:09,805] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:12,803] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:14,257] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:20,429] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:22,004] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:27,002] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:27,093] {spark_submit.py:514} INFO - 21/12/18 10:26:27 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(1,WrappedArray(),Map()) by listener AppStatusListener took 1.0719309s.
[2021-12-18 10:26:29,163] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:31,776] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:33,593] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:35,228] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:37,433] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:38,202] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:39,217] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:39,930] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:40,779] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:41,588] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:42,173] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:42,973] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:43,888] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:44,811] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:45,729] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:46,625] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:47,583] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:48,513] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:49,106] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:49,573] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:50,194] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:50,865] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:51,466] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:52,533] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:54,802] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:55,280] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:55,896] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:56,628] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:57,024] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:57,651] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:58,329] {spark_submit.py:514} INFO - running...
[2021-12-18 10:26:59,143] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:00,410] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:01,451] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:02,045] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:03,016] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:03,803] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:05,208] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:06,101] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:06,918] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:07,588] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:08,115] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:08,932] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:09,446] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:11,559] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:12,241] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:13,098] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:14,070] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:14,969] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:16,018] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:16,601] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:17,600] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:18,326] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:19,350] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:20,213] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:20,966] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:21,483] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:22,464] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:23,471] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:24,021] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:24,956] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:26,200] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:27,256] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:28,099] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:29,866] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:31,274] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:31,825] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:32,533] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:33,263] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:34,303] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:35,075] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:35,788] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:36,572] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:37,214] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:37,785] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:38,885] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:39,706] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:40,280] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:41,058] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:41,936] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:42,503] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:43,566] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:44,144] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:45,037] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:46,687] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:47,611] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:48,471] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:49,369] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:50,451] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:51,721] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:53,069] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:54,410] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:55,493] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:56,281] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:57,807] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:58,403] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:59,145] {spark_submit.py:514} INFO - running...
[2021-12-18 10:27:59,747] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:00,444] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:00,968] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:01,899] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:02,474] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:03,305] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:04,039] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:05,942] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:06,457] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:07,267] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:08,069] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:09,083] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:09,807] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:10,670] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:11,515] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:12,064] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:12,558] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:13,316] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:13,920] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:14,427] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:15,127] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:16,279] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:16,950] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:17,732] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:18,825] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:19,356] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:20,200] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:22,064] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:22,555] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:23,052] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:23,545] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:24,096] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:25,089] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:25,601] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:26,299] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:27,125] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:27,630] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:28,125] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:28,991] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:29,806] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:30,668] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:31,344] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:31,837] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:32,495] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:32,982] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:33,388] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:34,181] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:35,741] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:36,582] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:37,042] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:37,534] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:38,112] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:38,786] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:39,381] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:40,086] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:41,493] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:42,993] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:44,078] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:44,945] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:45,781] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:46,475] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:46,889] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:47,567] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:48,115] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:48,970] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:49,809] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:50,553] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:52,046] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:52,508] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:52,943] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:53,290] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:54,018] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:54,489] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:54,983] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:56,121] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:56,632] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:57,421] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:58,093] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:59,154] {spark_submit.py:514} INFO - running...
[2021-12-18 10:28:59,821] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:00,568] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:01,145] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:01,775] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:02,949] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:03,564] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:04,153] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:04,918] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:06,883] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:07,605] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:08,015] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:09,077] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:09,780] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:10,595] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:11,140] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:11,733] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:12,469] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:13,145] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:13,740] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:14,585] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:15,176] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:16,186] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:16,918] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:17,820] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:18,824] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:19,595] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:20,528] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:21,332] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:23,005] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:23,899] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:24,378] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:25,060] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:25,715] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:26,570] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:27,651] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:28,387] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:29,149] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:29,632] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:30,111] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:30,874] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:31,507] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:32,175] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:32,640] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:33,447] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:34,238] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:34,892] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:35,514] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:36,433] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:37,963] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:38,587] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:39,094] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:39,534] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:39,995] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:41,014] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:41,477] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:42,218] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:43,053] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:43,755] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:44,369] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:44,934] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:45,836] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:46,626] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:47,105] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:47,578] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:48,390] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:48,886] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:49,405] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:50,402] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:52,026] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:52,566] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:53,317] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:53,849] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:54,521] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:55,229] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:56,337] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:57,201] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:57,964] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:58,847] {spark_submit.py:514} INFO - running...
[2021-12-18 10:29:59,972] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:00,613] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:01,097] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:02,084] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:02,874] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:03,466] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:04,139] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:04,980] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:05,837] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:06,341] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:07,816] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:08,925] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:10,093] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:10,857] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:11,692] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:12,382] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:12,983] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:13,683] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:14,320] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:15,002] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:15,703] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:16,981] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:17,715] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:18,527] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:19,366] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:20,255] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:21,060] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:21,959] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:22,639] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:23,165] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:24,405] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:25,143] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:25,951] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:26,607] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:27,402] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:27,922] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:28,439] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:28,935] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:29,726] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:30,420] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:31,298] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:31,812] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:32,358] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:32,961] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:33,514] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:34,083] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:34,643] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:35,838] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:36,536] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:37,212] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:38,642] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:39,376] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:40,257] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:41,216] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:41,956] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:42,739] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:43,429] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:43,985] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:44,741] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:45,444] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:46,565] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:47,237] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:47,851] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:48,426] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:48,949] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:49,482] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:50,395] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:51,301] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:52,120] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:52,912] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:54,132] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:54,595] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:55,103] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:55,823] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:56,265] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:56,697] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:57,591] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:58,643] {spark_submit.py:514} INFO - running...
[2021-12-18 10:30:59,501] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:00,380] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:01,109] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:01,578] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:02,291] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:03,111] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:03,545] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:03,958] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:04,406] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:04,936] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:05,687] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:06,340] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:08,277] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:08,945] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:09,798] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:10,605] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:11,142] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:11,842] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:12,524] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:13,446] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:14,324] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:15,160] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:15,935] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:16,467] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:17,149] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:18,025] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:18,502] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:19,212] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:20,051] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:20,665] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:21,145] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:21,798] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:23,347] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:23,883] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:24,622] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:25,141] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:25,624] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:26,547] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:27,161] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:27,904] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:28,389] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:29,189] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:30,142] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:30,663] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:31,345] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:31,852] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:32,518] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:33,009] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:33,529] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:34,528] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:35,424] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:35,899] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:37,045] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:37,740] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:38,694] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:39,387] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:39,982] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:40,658] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:41,221] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:41,746] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:42,197] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:42,997] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:43,601] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:44,491] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:45,315] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:46,133] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:46,694] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:47,114] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:47,899] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:48,983] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:49,788] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:50,409] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:52,023] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:52,424] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:53,016] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:53,459] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:54,052] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:55,346] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:55,886] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:56,308] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:56,861] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:57,561] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:58,235] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:58,725] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:59,186] {spark_submit.py:514} INFO - running...
[2021-12-18 10:31:59,849] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:00,424] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:01,249] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:01,792] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:02,216] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:03,133] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:03,593] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:05,038] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:05,882] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:07,194] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:07,871] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:08,383] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:09,027] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:09,526] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:10,369] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:11,355] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:12,222] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:12,696] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:13,194] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:14,041] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:14,555] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:15,019] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:15,749] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:16,211] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:17,038] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:17,780] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:18,402] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:20,177] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:20,878] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:21,398] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:21,963] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:22,649] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:23,092] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:23,543] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:24,309] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:25,003] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:25,687] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:26,350] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:26,766] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:27,539] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:28,026] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:28,578] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:29,028] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:30,151] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:30,709] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:31,867] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:32,638] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:34,428] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:35,559] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:36,238] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:36,762] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:37,510] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:38,239] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:38,864] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:39,553] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:40,011] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:40,448] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:41,129] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:41,727] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:42,519] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:43,117] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:43,637] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:44,317] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:44,994] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:45,563] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:46,509] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:47,275] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:48,992] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:49,757] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:50,616] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:51,088] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:51,680] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:52,147] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:52,828] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:53,378] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:53,945] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:54,810] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:55,455] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:55,951] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:56,549] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:57,153] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:58,110] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:58,659] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:59,352] {spark_submit.py:514} INFO - running...
[2021-12-18 10:32:59,823] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:00,740] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:01,293] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:02,409] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:03,209] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:03,572] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:04,414] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:05,124] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:05,858] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:06,552] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:07,205] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:07,831] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:08,317] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:08,799] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:09,795] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:10,335] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:11,095] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:11,572] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:12,078] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:12,858] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:13,622] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:14,144] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:14,685] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:16,511] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:17,005] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:17,565] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:18,057] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:18,943] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:19,810] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:20,349] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:21,253] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:21,801] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:22,654] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:23,389] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:24,142] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:24,673] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:25,463] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:26,307] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:27,205] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:27,795] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:28,265] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:28,892] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:29,564] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:31,086] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:31,917] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:32,336] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:33,043] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:34,078] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:34,814] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:35,537] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:36,288] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:36,794] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:37,223] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:37,882] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:38,564] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:39,284] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:39,721] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:40,527] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:42,401] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:44,015] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:45,028] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:46,272] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:47,078] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:48,828] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:49,823] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:50,799] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:51,688] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:52,867] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:53,647] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:54,542] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:55,039] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:55,654] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:56,375] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:57,645] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:58,356] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:59,067] {spark_submit.py:514} INFO - running...
[2021-12-18 10:33:59,660] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:00,819] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:01,398] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:01,987] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:02,502] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:03,349] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:03,815] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:05,440] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:06,129] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:06,682] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:07,389] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:08,134] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:09,198] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:09,666] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:10,413] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:11,245] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:11,836] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:12,703] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:13,452] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:14,102] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:14,816] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:15,686] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:16,275] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:16,805] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:17,505] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:18,053] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:18,962] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:20,977] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:21,538] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:22,411] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:23,461] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:24,119] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:24,937] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:25,535] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:26,308] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:27,187] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:27,884] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:28,484] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:29,553] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:30,182] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:30,687] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:31,179] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:32,234] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:32,789] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:33,419] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:33,867] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:34,528] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:36,000] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:36,563] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:37,034] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:37,572] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:38,254] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:38,833] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:39,856] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:40,737] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:41,311] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:41,977] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:43,118] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:43,919] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:44,891] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:45,362] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:45,845] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:46,598] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:47,724] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:48,446] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:48,976] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:49,668] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:51,050] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:51,741] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:52,768] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:53,193] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:53,818] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:55,062] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:55,822] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:56,518] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:57,165] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:57,756] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:58,557] {spark_submit.py:514} INFO - running...
[2021-12-18 10:34:59,240] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:00,420] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:00,952] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:01,601] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:02,732] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:03,532] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:04,333] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:05,172] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:05,712] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:07,214] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:08,102] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:08,892] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:09,615] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:10,452] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:11,099] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:12,006] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:12,588] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:13,141] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:13,722] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:14,238] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:14,729] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:15,131] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:16,267] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:17,366] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:18,478] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:19,174] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:20,084] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:20,982] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:22,041] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:23,833] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:25,193] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:26,028] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:27,033] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:28,024] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:28,895] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:29,738] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:30,376] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:31,231] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:31,928] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:32,887] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:33,554] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:34,329] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:35,355] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:36,630] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:37,294] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:37,836] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:38,618] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:39,610] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:40,111] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:41,715] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:42,457] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:43,407] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:44,247] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:45,027] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:45,471] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:46,395] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:47,319] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:48,233] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:48,826] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:49,448] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:50,164] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:51,036] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:51,416] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:52,456] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:53,155] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:53,965] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:54,767] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:55,225] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:55,982] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:57,526] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:58,284] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:59,048] {spark_submit.py:514} INFO - running...
[2021-12-18 10:35:59,695] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:00,133] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:00,616] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:01,334] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:02,027] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:02,717] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:03,142] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:03,680] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:04,585] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:05,308] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:06,147] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:07,214] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:08,136] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:09,001] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:09,918] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:10,646] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:11,338] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:12,963] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:14,048] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:14,883] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:15,496] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:16,477] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:17,219] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:17,989] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:18,681] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:19,584] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:20,460] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:21,105] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:21,941] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:22,725] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:23,281] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:23,964] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:24,527] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:25,011] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:25,972] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:26,484] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:27,308] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:29,169] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:29,996] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:30,912] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:31,736] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:32,602] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:33,074] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:33,540] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:34,029] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:35,255] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:35,687] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:36,428] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:37,073] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:37,742] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:38,791] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:39,234] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:40,249] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:40,819] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:41,380] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:41,861] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:43,098] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:44,774] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:45,227] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:45,740] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:46,602] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:47,250] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:48,025] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:48,872] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:49,406] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:50,065] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:50,484] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:51,301] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:51,750] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:52,283] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:53,038] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:53,474] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:54,654] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:55,530] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:56,337] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:57,323] {spark_submit.py:514} INFO - running...
[2021-12-18 10:36:58,159] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:00,074] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:01,013] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:01,917] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:02,336] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:03,038] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:04,151] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:05,070] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:05,895] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:06,374] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:06,787] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:07,308] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:08,138] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:08,624] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:09,245] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:10,150] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:11,398] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:11,889] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:12,299] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:12,877] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:13,452] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:15,243] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:16,125] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:16,901] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:17,427] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:18,176] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:18,743] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:19,501] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:20,130] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:20,990] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:21,852] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:22,734] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:23,449] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:24,209] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:24,952] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:25,510] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:26,181] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:26,812] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:27,542] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:28,177] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:28,878] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:30,668] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:31,474] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:32,318] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:33,216] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:33,958] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:34,968] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:35,636] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:35,990] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:36,945] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:37,428] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:37,948] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:38,925] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:39,746] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:40,545] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:41,161] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:41,696] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:42,404] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:43,186] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:43,636] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:44,317] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:46,131] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:46,840] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:47,701] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:48,316] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:48,989] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:49,753] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:50,425] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:50,959] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:51,562] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:52,336] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:52,818] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:53,571] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:54,575] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:55,117] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:55,847] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:56,499] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:57,262] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:57,782] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:58,258] {spark_submit.py:514} INFO - running...
[2021-12-18 10:37:58,804] {spark_submit.py:514} INFO - running...
[2021-12-18 10:38:00,497] {spark_submit.py:514} INFO - running...
[2021-12-18 10:38:01,519] {spark_submit.py:514} INFO - running...
[2021-12-18 10:38:02,453] {spark_submit.py:514} INFO - running...
[2021-12-18 10:38:03,005] {spark_submit.py:514} INFO - running...
[2021-12-18 10:38:03,566] {spark_submit.py:514} INFO - running...
[2021-12-18 10:38:04,287] {spark_submit.py:514} INFO - running...
[2021-12-18 10:38:05,333] {spark_submit.py:514} INFO - running...
[2021-12-18 10:38:06,254] {spark_submit.py:514} INFO - running...
[2021-12-18 10:38:07,003] {spark_submit.py:514} INFO - running...
[2021-12-18 10:38:08,668] {spark_submit.py:514} INFO - 21/12/18 10:38:08 INFO SparkContext: Invoking stop() from shutdown hook
[2021-12-18 10:38:08,754] {spark_submit.py:514} INFO - 21/12/18 10:38:08 INFO SparkUI: Stopped Spark web UI at http://cf1c44b514e9:4045
[2021-12-18 10:38:08,771] {spark_submit.py:514} INFO - 21/12/18 10:38:08 INFO StandaloneSchedulerBackend: Shutting down all executors
[2021-12-18 10:38:08,774] {spark_submit.py:514} INFO - 21/12/18 10:38:08 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
[2021-12-18 10:38:08,885] {spark_submit.py:514} INFO - 21/12/18 10:38:08 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2021-12-18 10:38:08,964] {spark_submit.py:514} INFO - 21/12/18 10:38:08 INFO MemoryStore: MemoryStore cleared
[2021-12-18 10:38:08,967] {spark_submit.py:514} INFO - 21/12/18 10:38:08 INFO BlockManager: BlockManager stopped
[2021-12-18 10:38:08,984] {spark_submit.py:514} INFO - 21/12/18 10:38:08 INFO BlockManagerMaster: BlockManagerMaster stopped
[2021-12-18 10:38:08,992] {spark_submit.py:514} INFO - 21/12/18 10:38:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2021-12-18 10:38:09,077] {spark_submit.py:514} INFO - 21/12/18 10:38:09 INFO SparkContext: Successfully stopped SparkContext
[2021-12-18 10:38:09,079] {spark_submit.py:514} INFO - 21/12/18 10:38:09 INFO ShutdownHookManager: Shutdown hook called
[2021-12-18 10:38:09,084] {spark_submit.py:514} INFO - 21/12/18 10:38:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-c6471bfd-9da6-477c-8eb7-919fc098c5f7
[2021-12-18 10:38:09,089] {spark_submit.py:514} INFO - 21/12/18 10:38:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-f91a0a36-e6b2-40fd-a289-8fec57c2c1dd
[2021-12-18 10:38:09,095] {spark_submit.py:514} INFO - 21/12/18 10:38:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-c6471bfd-9da6-477c-8eb7-919fc098c5f7/pyspark-13d1e3f8-b420-485f-bf02-e6d3e37118a1
[2021-12-18 10:38:09,939] {taskinstance.py:1280} INFO - Marking task as SUCCESS. dag_id=scraping_daft_dublin, task_id=scrap_data, execution_date=20211217T093000, start_date=20211218T102008, end_date=20211218T103809
[2021-12-18 10:38:10,081] {local_task_job.py:154} INFO - Task exited with return code 0
[2021-12-18 10:38:10,417] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
