[2022-01-04 12:32:18,204] {taskinstance.py:1035} INFO - Dependencies all met for <TaskInstance: scraping_daft_dublin.scrap_data scheduled__2022-01-03T09:30:00+00:00 [queued]>
[2022-01-04 12:32:18,369] {taskinstance.py:1035} INFO - Dependencies all met for <TaskInstance: scraping_daft_dublin.scrap_data scheduled__2022-01-03T09:30:00+00:00 [queued]>
[2022-01-04 12:32:18,376] {taskinstance.py:1241} INFO - 
--------------------------------------------------------------------------------
[2022-01-04 12:32:18,381] {taskinstance.py:1242} INFO - Starting attempt 3 of 4
[2022-01-04 12:32:18,387] {taskinstance.py:1243} INFO - 
--------------------------------------------------------------------------------
[2022-01-04 12:32:18,530] {taskinstance.py:1262} INFO - Executing <Task(SparkSubmitOperator): scrap_data> on 2022-01-03 09:30:00+00:00
[2022-01-04 12:32:18,762] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'scraping_daft_dublin', 'scrap_data', 'scheduled__2022-01-03T09:30:00+00:00', '--job-id', '6640', '--raw', '--subdir', 'DAGS_FOLDER/scraping_daft_dublin.py', '--cfg-path', '/tmp/tmprarvriy3', '--error-file', '/tmp/tmpar48rhmz']
[2022-01-04 12:32:18,772] {standard_task_runner.py:77} INFO - Job 6640: Subtask scrap_data
[2022-01-04 12:32:18,686] {standard_task_runner.py:52} INFO - Started process 13219 to run task
[2022-01-04 12:32:20,051] {logging_mixin.py:109} INFO - Running <TaskInstance: scraping_daft_dublin.scrap_data scheduled__2022-01-03T09:30:00+00:00 [running]> on host 60c978034afe
[2022-01-04 12:32:22,181] {taskinstance.py:1429} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_EMAIL=clement.liscoet@gmail.com
AIRFLOW_CTX_DAG_OWNER=clemoni
AIRFLOW_CTX_DAG_ID=scraping_daft_dublin
AIRFLOW_CTX_TASK_ID=scrap_data
AIRFLOW_CTX_EXECUTION_DATE=2022-01-03T09:30:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-01-03T09:30:00+00:00
[2022-01-04 12:32:22,233] {base.py:79} INFO - Using connection to: id: spark_default. Host: spark://spark:7077, Port: None, Schema: , Login: ***, Password: ***, extra: {}
[2022-01-04 12:32:22,310] {spark_submit.py:360} INFO - Spark-Submit cmd: spark-submit --master spark://spark:7077 --conf spark.master=spark://spark:7077 --name scrap data --verbose /opt/***/jobs/get_ads_per_county.py dublin
[2022-01-04 12:33:04,297] {spark_submit.py:514} INFO - Using properties file: null
[2022-01-04 12:33:05,595] {spark_submit.py:514} INFO - WARNING: An illegal reflective access operation has occurred
[2022-01-04 12:33:05,596] {spark_submit.py:514} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/***/.local/lib/python3.6/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2022-01-04 12:33:05,598] {spark_submit.py:514} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2022-01-04 12:33:05,600] {spark_submit.py:514} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2022-01-04 12:33:05,604] {spark_submit.py:514} INFO - WARNING: All illegal access operations will be denied in a future release
[2022-01-04 12:33:06,167] {spark_submit.py:514} INFO - Parsed arguments:
[2022-01-04 12:33:06,169] {spark_submit.py:514} INFO - master                  spark://spark:7077
[2022-01-04 12:33:06,170] {spark_submit.py:514} INFO - deployMode              null
[2022-01-04 12:33:06,173] {spark_submit.py:514} INFO - executorMemory          null
[2022-01-04 12:33:06,175] {spark_submit.py:514} INFO - executorCores           null
[2022-01-04 12:33:06,177] {spark_submit.py:514} INFO - totalExecutorCores      null
[2022-01-04 12:33:06,186] {spark_submit.py:514} INFO - propertiesFile          null
[2022-01-04 12:33:06,188] {spark_submit.py:514} INFO - driverMemory            null
[2022-01-04 12:33:06,190] {spark_submit.py:514} INFO - driverCores             null
[2022-01-04 12:33:06,195] {spark_submit.py:514} INFO - driverExtraClassPath    null
[2022-01-04 12:33:06,198] {spark_submit.py:514} INFO - driverExtraLibraryPath  null
[2022-01-04 12:33:06,202] {spark_submit.py:514} INFO - driverExtraJavaOptions  null
[2022-01-04 12:33:06,203] {spark_submit.py:514} INFO - supervise               false
[2022-01-04 12:33:06,206] {spark_submit.py:514} INFO - queue                   null
[2022-01-04 12:33:06,207] {spark_submit.py:514} INFO - numExecutors            null
[2022-01-04 12:33:06,209] {spark_submit.py:514} INFO - files                   null
[2022-01-04 12:33:06,212] {spark_submit.py:514} INFO - pyFiles                 null
[2022-01-04 12:33:06,215] {spark_submit.py:514} INFO - archives                null
[2022-01-04 12:33:06,218] {spark_submit.py:514} INFO - mainClass               null
[2022-01-04 12:33:06,220] {spark_submit.py:514} INFO - primaryResource         file:/opt/***/jobs/get_ads_per_county.py
[2022-01-04 12:33:06,222] {spark_submit.py:514} INFO - name                    scrap data
[2022-01-04 12:33:06,223] {spark_submit.py:514} INFO - childArgs               [dublin]
[2022-01-04 12:33:06,224] {spark_submit.py:514} INFO - jars                    null
[2022-01-04 12:33:06,226] {spark_submit.py:514} INFO - packages                null
[2022-01-04 12:33:06,227] {spark_submit.py:514} INFO - packagesExclusions      null
[2022-01-04 12:33:06,229] {spark_submit.py:514} INFO - repositories            null
[2022-01-04 12:33:06,230] {spark_submit.py:514} INFO - verbose                 true
[2022-01-04 12:33:06,232] {spark_submit.py:514} INFO - 
[2022-01-04 12:33:06,233] {spark_submit.py:514} INFO - Spark properties used, including those specified through
[2022-01-04 12:33:06,234] {spark_submit.py:514} INFO - --conf and those from the properties file null:
[2022-01-04 12:33:06,236] {spark_submit.py:514} INFO - (spark.master,spark://spark:7077)
[2022-01-04 12:33:06,237] {spark_submit.py:514} INFO - 
[2022-01-04 12:33:06,239] {spark_submit.py:514} INFO - 
[2022-01-04 12:33:12,817] {spark_submit.py:514} INFO - Main class:
[2022-01-04 12:33:12,820] {spark_submit.py:514} INFO - org.apache.spark.deploy.PythonRunner
[2022-01-04 12:33:12,824] {spark_submit.py:514} INFO - Arguments:
[2022-01-04 12:33:12,826] {spark_submit.py:514} INFO - file:/opt/***/jobs/get_ads_per_county.py
[2022-01-04 12:33:12,828] {spark_submit.py:514} INFO - null
[2022-01-04 12:33:12,832] {spark_submit.py:514} INFO - dublin
[2022-01-04 12:33:12,834] {spark_submit.py:514} INFO - --verbose
[2022-01-04 12:33:12,851] {spark_submit.py:514} INFO - Spark config:
[2022-01-04 12:33:12,855] {spark_submit.py:514} INFO - (spark.app.name,scrap data)
[2022-01-04 12:33:12,859] {spark_submit.py:514} INFO - (spark.master,spark://spark:7077)
[2022-01-04 12:33:12,862] {spark_submit.py:514} INFO - (spark.submit.pyFiles,)
[2022-01-04 12:33:12,864] {spark_submit.py:514} INFO - (spark.submit.deployMode,client)
[2022-01-04 12:33:12,868] {spark_submit.py:514} INFO - Classpath elements:
[2022-01-04 12:33:12,871] {spark_submit.py:514} INFO - 
[2022-01-04 12:33:12,874] {spark_submit.py:514} INFO - 
[2022-01-04 12:33:12,878] {spark_submit.py:514} INFO - 
[2022-01-04 12:34:02,735] {spark_submit.py:514} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2022-01-04 12:34:03,113] {spark_submit.py:514} INFO - 22/01/04 12:34:03 INFO SparkContext: Running Spark version 3.2.0
[2022-01-04 12:34:05,088] {spark_submit.py:514} INFO - 22/01/04 12:34:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-01-04 12:34:11,207] {spark_submit.py:514} INFO - 22/01/04 12:34:11 INFO ResourceUtils: ==============================================================
[2022-01-04 12:34:11,252] {spark_submit.py:514} INFO - 22/01/04 12:34:11 INFO ResourceUtils: No custom resources configured for spark.driver.
[2022-01-04 12:34:11,261] {spark_submit.py:514} INFO - 22/01/04 12:34:11 INFO ResourceUtils: ==============================================================
[2022-01-04 12:34:11,285] {spark_submit.py:514} INFO - 22/01/04 12:34:11 INFO SparkContext: Submitted application: Get Ads per county
[2022-01-04 12:34:13,860] {spark_submit.py:514} INFO - 22/01/04 12:34:13 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2022-01-04 12:34:14,017] {spark_submit.py:514} INFO - 22/01/04 12:34:14 INFO ResourceProfile: Limiting resource is cpu
[2022-01-04 12:34:14,026] {spark_submit.py:514} INFO - 22/01/04 12:34:14 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2022-01-04 12:34:16,401] {spark_submit.py:514} INFO - 22/01/04 12:34:16 INFO SecurityManager: Changing view acls to: ***
[2022-01-04 12:34:16,408] {spark_submit.py:514} INFO - 22/01/04 12:34:16 INFO SecurityManager: Changing modify acls to: ***
[2022-01-04 12:34:16,416] {spark_submit.py:514} INFO - 22/01/04 12:34:16 INFO SecurityManager: Changing view acls groups to:
[2022-01-04 12:34:16,421] {spark_submit.py:514} INFO - 22/01/04 12:34:16 INFO SecurityManager: Changing modify acls groups to:
[2022-01-04 12:34:16,423] {spark_submit.py:514} INFO - 22/01/04 12:34:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2022-01-04 12:34:22,022] {spark_submit.py:514} INFO - 22/01/04 12:34:21 INFO Utils: Successfully started service 'sparkDriver' on port 40893.
[2022-01-04 12:34:22,747] {spark_submit.py:514} INFO - 22/01/04 12:34:22 INFO SparkEnv: Registering MapOutputTracker
[2022-01-04 12:34:24,244] {spark_submit.py:514} INFO - 22/01/04 12:34:24 INFO SparkEnv: Registering BlockManagerMaster
[2022-01-04 12:34:24,623] {spark_submit.py:514} INFO - 22/01/04 12:34:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-01-04 12:34:24,625] {spark_submit.py:514} INFO - 22/01/04 12:34:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-01-04 12:34:25,226] {spark_submit.py:514} INFO - 22/01/04 12:34:25 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-01-04 12:34:25,976] {spark_submit.py:514} INFO - 22/01/04 12:34:25 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-656a3f1f-9521-4cd6-a98e-6a8bac2545e2
[2022-01-04 12:34:26,388] {spark_submit.py:514} INFO - 22/01/04 12:34:26 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2022-01-04 12:34:26,695] {spark_submit.py:514} INFO - 22/01/04 12:34:26 INFO SparkEnv: Registering OutputCommitCoordinator
[2022-01-04 12:34:34,174] {spark_submit.py:514} INFO - 22/01/04 12:34:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2022-01-04 12:34:34,175] {spark_submit.py:514} INFO - 22/01/04 12:34:34 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2022-01-04 12:34:34,182] {spark_submit.py:514} INFO - 22/01/04 12:34:34 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2022-01-04 12:34:34,183] {spark_submit.py:514} INFO - 22/01/04 12:34:34 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2022-01-04 12:34:34,199] {spark_submit.py:514} INFO - 22/01/04 12:34:34 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2022-01-04 12:34:34,202] {spark_submit.py:514} INFO - 22/01/04 12:34:34 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[2022-01-04 12:34:34,203] {spark_submit.py:514} INFO - 22/01/04 12:34:34 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
[2022-01-04 12:34:34,205] {spark_submit.py:514} INFO - 22/01/04 12:34:34 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.
[2022-01-04 12:34:34,740] {spark_submit.py:514} INFO - 22/01/04 12:34:34 INFO Utils: Successfully started service 'SparkUI' on port 4048.
[2022-01-04 12:34:36,238] {spark_submit.py:514} INFO - 22/01/04 12:34:36 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://60c978034afe:4048
[2022-01-04 12:34:40,978] {spark_submit.py:514} INFO - 22/01/04 12:34:40 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark:7077...
[2022-01-04 12:34:41,473] {spark_submit.py:514} INFO - 22/01/04 12:34:41 INFO TransportClientFactory: Successfully created connection to spark/172.23.0.4:7077 after 215 ms (0 ms spent in bootstraps)
[2022-01-04 12:34:41,969] {spark_submit.py:514} INFO - 22/01/04 12:34:41 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20220104123441-0084
[2022-01-04 12:34:42,091] {spark_submit.py:514} INFO - 22/01/04 12:34:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37347.
[2022-01-04 12:34:42,095] {spark_submit.py:514} INFO - 22/01/04 12:34:42 INFO NettyBlockTransferService: Server created on 60c978034afe:37347
[2022-01-04 12:34:42,096] {spark_submit.py:514} INFO - 22/01/04 12:34:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-01-04 12:34:42,216] {spark_submit.py:514} INFO - 22/01/04 12:34:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 60c978034afe, 37347, None)
[2022-01-04 12:34:42,266] {spark_submit.py:514} INFO - 22/01/04 12:34:42 INFO BlockManagerMasterEndpoint: Registering block manager 60c978034afe:37347 with 434.4 MiB RAM, BlockManagerId(driver, 60c978034afe, 37347, None)
[2022-01-04 12:34:42,293] {spark_submit.py:514} INFO - 22/01/04 12:34:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 60c978034afe, 37347, None)
[2022-01-04 12:34:42,297] {spark_submit.py:514} INFO - 22/01/04 12:34:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 60c978034afe, 37347, None)
[2022-01-04 12:34:44,524] {spark_submit.py:514} INFO - 22/01/04 12:34:44 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2022-01-04 12:34:44,729] {spark_submit.py:514} INFO - /home/***/.local/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.
[2022-01-04 12:34:44,730] {spark_submit.py:514} INFO - FutureWarning
[2022-01-04 12:34:49,375] {spark_submit.py:514} INFO - 22/01/04 12:34:49 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2022-01-04 12:34:49,727] {spark_submit.py:514} INFO - 22/01/04 12:34:49 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2022-01-04 12:35:19,043] {spark_submit.py:514} INFO - running...
[2022-01-04 12:35:24,763] {spark_submit.py:514} INFO - 22/01/04 12:35:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220104123441-0084/0 on worker-20220104120454-172.23.0.5-35099 (172.23.0.5:35099) with 6 core(s)
[2022-01-04 12:35:24,833] {spark_submit.py:514} INFO - 22/01/04 12:35:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20220104123441-0084/0 on hostPort 172.23.0.5:35099 with 6 core(s), 1024.0 MiB RAM
[2022-01-04 12:35:24,861] {spark_submit.py:514} INFO - 22/01/04 12:35:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220104123441-0084/1 on worker-20220104120454-172.23.0.6-42513 (172.23.0.6:42513) with 6 core(s)
[2022-01-04 12:35:24,870] {spark_submit.py:514} INFO - 22/01/04 12:35:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20220104123441-0084/1 on hostPort 172.23.0.6:42513 with 6 core(s), 1024.0 MiB RAM
[2022-01-04 12:35:25,489] {spark_submit.py:514} INFO - running...
[2022-01-04 12:35:25,812] {spark_submit.py:514} INFO - 22/01/04 12:35:25 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20220104123441-0084/0 is now RUNNING
[2022-01-04 12:35:25,814] {spark_submit.py:514} INFO - 22/01/04 12:35:25 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20220104123441-0084/1 is now RUNNING
[2022-01-04 12:35:30,182] {spark_submit.py:514} INFO - running...
[2022-01-04 12:35:32,401] {spark_submit.py:514} INFO - running...
[2022-01-04 12:35:38,195] {spark_submit.py:514} INFO - running...
[2022-01-04 12:35:40,869] {spark_submit.py:514} INFO - 22/01/04 12:35:40 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.5:48712) with ID 0,  ResourceProfileId 0
[2022-01-04 12:35:40,880] {spark_submit.py:514} INFO - 22/01/04 12:35:40 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.6:59490) with ID 1,  ResourceProfileId 0
[2022-01-04 12:35:41,533] {spark_submit.py:514} INFO - 22/01/04 12:35:41 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.6:46761 with 366.3 MiB RAM, BlockManagerId(1, 172.23.0.6, 46761, None)
[2022-01-04 12:35:41,593] {spark_submit.py:514} INFO - 22/01/04 12:35:41 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.5:46639 with 366.3 MiB RAM, BlockManagerId(0, 172.23.0.5, 46639, None)
[2022-01-04 12:35:42,408] {spark_submit.py:514} INFO - running...
[2022-01-04 12:35:45,175] {spark_submit.py:514} INFO - running...
[2022-01-04 12:35:48,387] {spark_submit.py:514} INFO - running...
[2022-01-04 12:35:50,421] {spark_submit.py:514} INFO - running...
[2022-01-04 12:35:51,349] {spark_submit.py:514} INFO - running...
[2022-01-04 12:35:53,768] {spark_submit.py:514} INFO - running...
[2022-01-04 12:35:56,645] {spark_submit.py:514} INFO - running...
[2022-01-04 12:35:59,649] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:01,528] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:02,945] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:06,083] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:09,143] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:10,029] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:11,031] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:12,360] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:15,941] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:16,818] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:17,425] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:18,908] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:19,596] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:19,598] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:20,720] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:22,091] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:23,577] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:25,590] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:27,978] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:29,350] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:31,064] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:32,621] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:33,223] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:34,890] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:35,448] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:35,975] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:37,199] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:38,753] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:41,113] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:41,874] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:42,900] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:44,170] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:45,928] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:46,501] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:47,257] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:48,235] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:49,654] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:50,573] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:51,041] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:52,991] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:53,682] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:54,497] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:55,511] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:56,328] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:58,244] {spark_submit.py:514} INFO - running...
[2022-01-04 12:36:58,873] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:00,390] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:02,077] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:05,478] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:06,555] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:09,802] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:11,466] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:12,901] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:14,555] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:16,264] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:17,251] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:18,177] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:18,766] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:19,531] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:20,750] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:21,568] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:22,140] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:22,734] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:23,168] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:24,318] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:24,802] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:25,628] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:26,608] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:28,429] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:29,317] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:30,346] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:30,856] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:31,563] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:32,054] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:32,703] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:33,335] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:34,156] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:34,761] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:35,319] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:36,097] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:37,053] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:38,780] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:39,671] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:41,211] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:42,368] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:43,202] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:44,218] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:44,840] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:46,289] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:46,749] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:47,312] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:48,036] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:48,649] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:49,476] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:50,424] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:51,043] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:51,985] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:52,727] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:53,815] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:54,506] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:55,105] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:56,133] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:57,294] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:58,249] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:58,960] {spark_submit.py:514} INFO - running...
[2022-01-04 12:37:59,710] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:00,614] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:02,604] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:04,369] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:05,254] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:06,210] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:07,143] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:09,085] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:10,191] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:11,026] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:11,526] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:12,133] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:13,191] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:14,074] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:14,610] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:15,128] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:15,642] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:16,221] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:17,096] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:18,020] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:18,448] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:19,142] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:19,835] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:21,381] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:22,053] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:22,829] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:23,288] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:24,062] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:24,906] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:25,824] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:26,542] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:27,331] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:28,370] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:29,087] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:29,927] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:30,644] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:31,388] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:32,169] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:32,681] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:33,449] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:34,064] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:34,601] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:35,210] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:38,712] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:39,742] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:40,241] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:41,074] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:41,633] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:42,462] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:42,952] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:43,876] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:44,850] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:45,516] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:46,380] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:47,687] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:48,846] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:49,832] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:50,440] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:51,482] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:52,211] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:52,976] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:53,662] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:54,320] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:56,099] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:56,966] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:57,482] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:58,052] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:59,010] {spark_submit.py:514} INFO - running...
[2022-01-04 12:38:59,635] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:00,172] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:00,957] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:01,497] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:02,040] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:02,652] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:03,357] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:04,443] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:05,409] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:06,735] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:08,261] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:09,508] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:10,391] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:11,285] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:12,125] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:14,127] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:14,670] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:15,508] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:16,058] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:16,534] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:17,431] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:18,341] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:19,127] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:20,303] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:21,542] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:22,339] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:22,914] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:23,513] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:24,232] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:24,963] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:25,727] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:26,666] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:27,205] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:28,502] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:29,336] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:32,371] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:33,023] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:33,637] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:34,321] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:35,117] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:35,769] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:36,724] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:38,363] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:39,374] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:40,345] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:41,160] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:41,780] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:42,580] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:43,692] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:44,410] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:45,080] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:45,564] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:46,317] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:47,391] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:47,931] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:49,486] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:50,071] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:50,645] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:51,192] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:51,726] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:52,381] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:53,174] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:53,787] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:54,312] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:55,134] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:55,913] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:56,631] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:57,230] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:57,919] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:58,551] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:59,184] {spark_submit.py:514} INFO - running...
[2022-01-04 12:39:59,743] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:00,408] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:00,912] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:01,685] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:04,208] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:05,062] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:05,546] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:06,260] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:07,909] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:09,114] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:10,235] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:10,999] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:11,831] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:12,564] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:13,502] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:14,312] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:15,156] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:16,146] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:16,813] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:18,117] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:18,697] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:19,327] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:19,848] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:20,579] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:22,846] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:23,521] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:24,035] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:24,541] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:25,329] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:26,000] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:26,559] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:27,539] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:28,353] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:28,937] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:29,477] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:30,132] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:31,105] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:31,703] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:32,752] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:33,261] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:34,322] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:34,954] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:35,753] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:37,428] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:40,253] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:41,484] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:42,233] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:42,764] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:43,324] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:43,880] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:44,406] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:45,149] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:45,720] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:46,512] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:46,993] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:47,433] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:48,313] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:49,006] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:49,909] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:50,710] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:51,458] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:52,296] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:53,070] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:53,900] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:55,471] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:56,268] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:57,247] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:57,918] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:59,011] {spark_submit.py:514} INFO - running...
[2022-01-04 12:40:59,885] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:00,636] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:01,593] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:02,204] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:03,073] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:03,748] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:04,572] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:05,410] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:06,233] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:07,877] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:09,267] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:09,999] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:10,911] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:11,865] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:12,671] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:15,404] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:16,400] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:17,200] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:17,724] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:18,270] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:18,991] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:19,749] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:20,419] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:21,249] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:22,056] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:23,177] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:24,203] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:24,887] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:25,798] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:26,828] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:27,684] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:28,384] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:29,496] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:30,577] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:31,429] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:33,551] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:34,455] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:35,691] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:37,347] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:38,935] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:39,956] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:40,858] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:41,485] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:42,285] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:43,244] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:44,273] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:45,056] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:45,637] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:46,239] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:46,654] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:47,390] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:47,951] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:48,554] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:49,607] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:50,222] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:52,254] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:52,982] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:54,438] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:55,108] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:55,557] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:56,094] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:56,821] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:57,339] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:57,934] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:58,518] {spark_submit.py:514} INFO - running...
[2022-01-04 12:41:59,664] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:00,475] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:01,033] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:01,985] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:02,578] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:03,121] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:03,765] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:04,396] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:05,176] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:05,708] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:10,066] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:10,838] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:11,415] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:11,997] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:12,629] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:13,539] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:14,169] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:14,912] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:15,952] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:16,701] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:17,623] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:18,365] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:19,090] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:20,532] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:21,330] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:21,874] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:22,699] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:23,396] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:24,112] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:24,898] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:26,600] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:27,838] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:28,530] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:29,276] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:29,993] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:30,568] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:31,526] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:32,447] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:32,816] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:33,356] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:34,120] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:34,727] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:35,445] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:36,709] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:37,928] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:39,481] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:40,033] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:40,581] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:41,095] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:41,796] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:43,592] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:44,164] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:44,926] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:45,596] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:46,194] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:47,063] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:47,927] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:48,754] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:49,623] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:51,103] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:52,234] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:52,984] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:53,802] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:54,721] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:55,444] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:56,436] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:56,973] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:57,513] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:58,074] {spark_submit.py:514} INFO - running...
[2022-01-04 12:42:58,690] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:00,049] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:00,617] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:01,314] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:02,053] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:03,191] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:04,469] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:05,529] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:06,864] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:08,594] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:09,720] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:10,663] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:11,516] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:12,188] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:13,164] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:14,015] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:14,515] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:14,983] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:15,733] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:16,296] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:16,926] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:18,832] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:19,550] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:20,364] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:20,948] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:21,628] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:22,174] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:22,831] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:23,339] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:24,087] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:24,662] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:25,321] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:25,850] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:26,589] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:27,328] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:27,946] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:28,820] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:29,448] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:30,256] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:30,714] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:31,496] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:33,623] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:42,631] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:43,794] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:44,473] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:45,272] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:46,038] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:46,971] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:47,635] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:48,146] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:48,839] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:50,413] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:51,977] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:53,221] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:54,482] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:54,962] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:55,904] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:56,737] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:57,479] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:58,297] {spark_submit.py:514} INFO - running...
[2022-01-04 12:43:59,111] {spark_submit.py:514} INFO - running...
[2022-01-04 12:44:01,103] {spark_submit.py:514} INFO - running...
[2022-01-04 12:44:02,224] {spark_submit.py:514} INFO - running...
[2022-01-04 12:44:03,100] {spark_submit.py:514} INFO - running...
[2022-01-04 12:44:03,792] {spark_submit.py:514} INFO - running...
[2022-01-04 12:44:04,752] {spark_submit.py:514} INFO - running...
[2022-01-04 12:44:05,447] {spark_submit.py:514} INFO - running...
[2022-01-04 12:44:06,823] {spark_submit.py:514} INFO - running...
[2022-01-04 12:44:08,321] {spark_submit.py:514} INFO - running...
[2022-01-04 12:44:09,748] {spark_submit.py:514} INFO - running...
[2022-01-04 12:44:10,089] {spark_submit.py:514} INFO - running...
[2022-01-04 12:44:11,124] {spark_submit.py:514} INFO - running...
[2022-01-04 12:44:11,693] {spark_submit.py:514} INFO - running...
[2022-01-04 12:44:12,387] {spark_submit.py:514} INFO - running...
[2022-01-04 12:44:13,250] {spark_submit.py:514} INFO - running...
[2022-01-04 12:44:14,366] {spark_submit.py:514} INFO - running...
[2022-01-04 12:44:15,470] {spark_submit.py:514} INFO - running...
[2022-01-04 12:45:46,972] {spark_submit.py:514} INFO - <urlopen error [Errno 0] Error>
[2022-01-04 12:45:47,026] {spark_submit.py:514} INFO - Traceback (most recent call last):
[2022-01-04 12:45:47,040] {spark_submit.py:514} INFO - File "/opt/***/jobs/get_ads_per_county.py", line 17, in <module>
[2022-01-04 12:45:47,095] {spark_submit.py:514} INFO - get_rent_parse_county_json(county)
[2022-01-04 12:45:47,112] {spark_submit.py:514} INFO - File "/opt/***/jobs/my_packages/fptools/fptools.py", line 191, in h
[2022-01-04 12:45:47,128] {spark_submit.py:514} INFO - return g(x, f(x))
[2022-01-04 12:45:47,158] {spark_submit.py:514} INFO - File "/opt/***/jobs/my_packages/main.py", line 32, in run_get_ads_by_county
[2022-01-04 12:45:47,180] {spark_submit.py:514} INFO - get_daft)(limit, county=county)
[2022-01-04 12:45:47,199] {spark_submit.py:514} INFO - File "/opt/***/jobs/my_packages/scraptools/scrap_main.py", line 254, in get_adds_by_page
[2022-01-04 12:45:47,215] {spark_submit.py:514} INFO - return get_adds_by_page(limit, county=county, page=next_page, res=res)
[2022-01-04 12:45:47,223] {spark_submit.py:514} INFO - File "/opt/***/jobs/my_packages/scraptools/scrap_main.py", line 254, in get_adds_by_page
[2022-01-04 12:45:47,233] {spark_submit.py:514} INFO - return get_adds_by_page(limit, county=county, page=next_page, res=res)
[2022-01-04 12:45:47,242] {spark_submit.py:514} INFO - File "/opt/***/jobs/my_packages/scraptools/scrap_main.py", line 254, in get_adds_by_page
[2022-01-04 12:45:47,253] {spark_submit.py:514} INFO - return get_adds_by_page(limit, county=county, page=next_page, res=res)
[2022-01-04 12:45:47,261] {spark_submit.py:514} INFO - [Previous line repeated 23 more times]
[2022-01-04 12:45:47,267] {spark_submit.py:514} INFO - File "/opt/***/jobs/my_packages/scraptools/scrap_main.py", line 252, in get_adds_by_page
[2022-01-04 12:45:47,307] {spark_submit.py:514} INFO - res.append(extract_and_get_data_ads(get_daft, url, county)(daft))
[2022-01-04 12:45:47,354] {spark_submit.py:514} INFO - File "/opt/***/jobs/my_packages/scraptools/scrap_main.py", line 193, in h
[2022-01-04 12:45:47,388] {spark_submit.py:514} INFO - return g(*args)(f(x))
[2022-01-04 12:45:47,408] {spark_submit.py:514} INFO - File "/opt/***/jobs/my_packages/scraptools/scrap_main.py", line 181, in get_data_ads
[2022-01-04 12:45:47,444] {spark_submit.py:514} INFO - res.append(parse_advert_from_daft(ad))
[2022-01-04 12:45:47,458] {spark_submit.py:514} INFO - File "/opt/***/jobs/my_packages/scraptools/scrap_main.py", line 120, in parse_advert
[2022-01-04 12:45:47,470] {spark_submit.py:514} INFO - ad_data = get_ad_data(ad)
[2022-01-04 12:45:47,486] {spark_submit.py:514} INFO - File "/opt/***/jobs/my_packages/fptools/fptools.py", line 184, in format_compose
[2022-01-04 12:45:47,524] {spark_submit.py:514} INFO - vals = [f(x) for f in funcs]
[2022-01-04 12:45:47,541] {spark_submit.py:514} INFO - File "/opt/***/jobs/my_packages/fptools/fptools.py", line 184, in <listcomp>
[2022-01-04 12:45:47,550] {spark_submit.py:514} INFO - vals = [f(x) for f in funcs]
[2022-01-04 12:45:47,562] {spark_submit.py:514} INFO - File "/opt/***/jobs/my_packages/fptools/fptools.py", line 35, in h
[2022-01-04 12:45:47,584] {spark_submit.py:514} INFO - return g(f(x))
[2022-01-04 12:45:47,601] {spark_submit.py:514} INFO - File "/opt/***/jobs/my_packages/scraptools/scrap_info.py", line 37, in extract_id
[2022-01-04 12:45:47,631] {spark_submit.py:514} INFO - return ad.find('p', {'class': re.compile('DaftIDText.')}).get_text()
[2022-01-04 12:45:47,638] {spark_submit.py:514} INFO - AttributeError: 'NoneType' object has no attribute 'find'
[2022-01-04 12:45:49,375] {spark_submit.py:514} INFO - 22/01/04 12:45:49 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20220104123441-0084/1 is now LOST (worker lost)
[2022-01-04 12:45:49,835] {spark_submit.py:514} INFO - 22/01/04 12:45:49 INFO StandaloneSchedulerBackend: Executor app-20220104123441-0084/1 removed: worker lost
[2022-01-04 12:45:49,871] {spark_submit.py:514} INFO - 22/01/04 12:45:49 INFO StandaloneAppClient$ClientEndpoint: Master removed worker worker-20220104120454-172.23.0.6-42513: Not receiving heartbeat for 60 seconds
[2022-01-04 12:45:49,907] {spark_submit.py:514} INFO - 22/01/04 12:45:49 INFO StandaloneSchedulerBackend: Worker worker-20220104120454-172.23.0.6-42513 removed: Not receiving heartbeat for 60 seconds
[2022-01-04 12:45:49,925] {spark_submit.py:514} INFO - 22/01/04 12:45:49 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20220104123441-0084/0 is now LOST (worker lost)
[2022-01-04 12:45:49,939] {spark_submit.py:514} INFO - 22/01/04 12:45:49 INFO StandaloneSchedulerBackend: Executor app-20220104123441-0084/0 removed: worker lost
[2022-01-04 12:45:49,952] {spark_submit.py:514} INFO - 22/01/04 12:45:49 INFO StandaloneAppClient$ClientEndpoint: Master removed worker worker-20220104120454-172.23.0.5-35099: Not receiving heartbeat for 60 seconds
[2022-01-04 12:45:49,966] {spark_submit.py:514} INFO - 22/01/04 12:45:49 INFO StandaloneSchedulerBackend: Worker worker-20220104120454-172.23.0.5-35099 removed: Not receiving heartbeat for 60 seconds
[2022-01-04 12:45:50,044] {spark_submit.py:514} INFO - 22/01/04 12:45:50 ERROR TaskSchedulerImpl: Lost executor 1 on 172.23.0.6: worker lost
[2022-01-04 12:45:50,240] {spark_submit.py:514} INFO - 22/01/04 12:45:50 INFO TaskSchedulerImpl: Handle removed worker worker-20220104120454-172.23.0.6-42513: Not receiving heartbeat for 60 seconds
[2022-01-04 12:45:50,416] {spark_submit.py:514} INFO - 22/01/04 12:45:50 ERROR TaskSchedulerImpl: Lost executor 0 on 172.23.0.5: worker lost
[2022-01-04 12:45:50,437] {spark_submit.py:514} INFO - 22/01/04 12:45:50 INFO TaskSchedulerImpl: Handle removed worker worker-20220104120454-172.23.0.5-35099: Not receiving heartbeat for 60 seconds
[2022-01-04 12:45:50,762] {spark_submit.py:514} INFO - 22/01/04 12:45:50 INFO DAGScheduler: Executor lost: 1 (epoch 0)
[2022-01-04 12:45:51,073] {spark_submit.py:514} INFO - 22/01/04 12:45:51 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220104123441-0084/2 on worker-20220104120454-172.23.0.6-42513 (172.23.0.6:42513) with 6 core(s)
[2022-01-04 12:45:51,133] {spark_submit.py:514} INFO - 22/01/04 12:45:51 INFO StandaloneSchedulerBackend: Granted executor ID app-20220104123441-0084/2 on hostPort 172.23.0.6:42513 with 6 core(s), 1024.0 MiB RAM
[2022-01-04 12:45:51,152] {spark_submit.py:514} INFO - 22/01/04 12:45:51 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220104123441-0084/3 on worker-20220104120454-172.23.0.5-35099 (172.23.0.5:35099) with 6 core(s)
[2022-01-04 12:45:51,190] {spark_submit.py:514} INFO - 22/01/04 12:45:51 INFO StandaloneSchedulerBackend: Granted executor ID app-20220104123441-0084/3 on hostPort 172.23.0.5:35099 with 6 core(s), 1024.0 MiB RAM
[2022-01-04 12:45:51,911] {spark_submit.py:514} INFO - 22/01/04 12:45:51 INFO BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
[2022-01-04 12:45:52,622] {spark_submit.py:514} INFO - 22/01/04 12:45:52 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, 172.23.0.6, 46761, None)
[2022-01-04 12:45:52,720] {spark_submit.py:514} INFO - 22/01/04 12:45:52 INFO BlockManagerMaster: Removed 1 successfully in removeExecutor
[2022-01-04 12:45:52,883] {spark_submit.py:514} INFO - 22/01/04 12:45:52 INFO DAGScheduler: Shuffle files lost for host: 172.23.0.6 (epoch 0)
[2022-01-04 12:45:53,003] {spark_submit.py:514} INFO - 22/01/04 12:45:52 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.6:46761 with 366.3 MiB RAM, BlockManagerId(1, 172.23.0.6, 46761, None)
[2022-01-04 12:45:53,521] {spark_submit.py:514} INFO - 22/01/04 12:45:53 INFO DAGScheduler: Shuffle files lost for worker worker-20220104120454-172.23.0.6-42513 on host 172.23.0.6
[2022-01-04 12:45:53,533] {spark_submit.py:514} INFO - 22/01/04 12:45:53 INFO DAGScheduler: Executor lost: 0 (epoch 2)
[2022-01-04 12:45:53,612] {spark_submit.py:514} INFO - 22/01/04 12:45:53 INFO BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
[2022-01-04 12:45:53,705] {spark_submit.py:514} INFO - 22/01/04 12:45:53 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(0, 172.23.0.5, 46639, None)
[2022-01-04 12:45:53,732] {spark_submit.py:514} INFO - 22/01/04 12:45:53 INFO BlockManagerMaster: Removed 0 successfully in removeExecutor
[2022-01-04 12:45:53,905] {spark_submit.py:514} INFO - 22/01/04 12:45:53 INFO DAGScheduler: Shuffle files lost for host: 172.23.0.5 (epoch 2)
[2022-01-04 12:45:53,928] {spark_submit.py:514} INFO - 22/01/04 12:45:53 INFO DAGScheduler: Shuffle files lost for worker worker-20220104120454-172.23.0.5-35099 on host 172.23.0.5
[2022-01-04 12:45:54,178] {spark_submit.py:514} INFO - 22/01/04 12:45:54 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20220104123441-0084/3 is now RUNNING
[2022-01-04 12:45:54,677] {spark_submit.py:514} INFO - 22/01/04 12:45:54 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20220104123441-0084/2 is now RUNNING
[2022-01-04 12:46:00,842] {spark_submit.py:514} INFO - 22/01/04 12:46:00 INFO SparkContext: Invoking stop() from shutdown hook
[2022-01-04 12:46:01,508] {spark_submit.py:514} INFO - 22/01/04 12:46:01 INFO SparkUI: Stopped Spark web UI at http://60c978034afe:4048
[2022-01-04 12:46:01,851] {spark_submit.py:514} INFO - 22/01/04 12:46:01 INFO StandaloneSchedulerBackend: Shutting down all executors
[2022-01-04 12:46:01,894] {spark_submit.py:514} INFO - 22/01/04 12:46:01 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
[2022-01-04 12:47:20,727] {spark_submit.py:514} INFO - 22/01/04 12:47:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2022-01-04 12:47:21,248] {spark_submit.py:514} INFO - 22/01/04 12:47:21 INFO MemoryStore: MemoryStore cleared
[2022-01-04 12:47:21,273] {spark_submit.py:514} INFO - 22/01/04 12:47:21 INFO BlockManager: BlockManager stopped
[2022-01-04 12:47:21,297] {spark_submit.py:514} INFO - 22/01/04 12:47:21 INFO BlockManagerMaster: BlockManagerMaster stopped
[2022-01-04 12:47:21,349] {spark_submit.py:514} INFO - 22/01/04 12:47:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2022-01-04 12:47:21,680] {spark_submit.py:514} INFO - 22/01/04 12:47:21 INFO SparkContext: Successfully stopped SparkContext
[2022-01-04 12:47:21,700] {spark_submit.py:514} INFO - 22/01/04 12:47:21 INFO ShutdownHookManager: Shutdown hook called
[2022-01-04 12:47:21,707] {spark_submit.py:514} INFO - 22/01/04 12:47:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-9b66c3e9-4bda-46fa-9983-462d5c1420f0
[2022-01-04 12:47:21,730] {spark_submit.py:514} INFO - 22/01/04 12:47:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-24f6c9bd-ab32-47d1-8248-fed2c1112137/pyspark-b47a12b5-9147-4ae9-9968-f8a162a06fe6
[2022-01-04 12:47:21,744] {spark_submit.py:514} INFO - 22/01/04 12:47:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-24f6c9bd-ab32-47d1-8248-fed2c1112137
[2022-01-04 12:47:34,590] {taskinstance.py:1703} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1332, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1458, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1514, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 179, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 446, in submit
    f"Cannot execute: {self._mask_cmd(spark_submit_cmd)}. Error code is: {returncode}."
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://spark:7077 --conf spark.master=spark://spark:7077 --name scrap data --verbose /opt/***/jobs/get_ads_per_county.py dublin. Error code is: 1.
[2022-01-04 12:47:34,991] {taskinstance.py:1280} INFO - Marking task as UP_FOR_RETRY. dag_id=scraping_daft_dublin, task_id=scrap_data, execution_date=20220103T093000, start_date=20220104T123218, end_date=20220104T124734
[2022-01-04 12:47:35,295] {standard_task_runner.py:91} ERROR - Failed to execute job 6640 for task scrap_data
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/task/task_runner/standard_task_runner.py", line 85, in _start_by_fork
    args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/cli.py", line 92, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py", line 292, in task_run
    _run_task_by_selected_method(args, dag, ti)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py", line 107, in _run_task_by_selected_method
    _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py", line 184, in _run_raw_task
    error_file=args.error_file,
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1332, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1458, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1514, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 179, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 446, in submit
    f"Cannot execute: {self._mask_cmd(spark_submit_cmd)}. Error code is: {returncode}."
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://spark:7077 --conf spark.master=spark://spark:7077 --name scrap data --verbose /opt/***/jobs/get_ads_per_county.py dublin. Error code is: 1.
[2022-01-04 12:47:36,144] {local_task_job.py:154} INFO - Task exited with return code 1
[2022-01-04 12:47:36,886] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
